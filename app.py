import streamlit as st
import pandas as pd
import numpy as np
import re
from io import BytesIO
from datetime import datetime
import os
import json

from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# --- å¤–éƒ¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from status import calculate_status 

# åŸºæº–æ—¥ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã®æ—¥ä»˜ã‚’å®šç¾©
TODAY = datetime.now().date()
TODAY_STR = TODAY.strftime('%Y%m%d')

def local_css(file_name):
    """å¤–éƒ¨CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®é–¢æ•°"""
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    except FileNotFoundError:
        st.error(f"CSSãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")

# --- ãƒšãƒ¼ã‚¸è¨­å®š (ãƒ¯ã‚¤ãƒ‰ãƒ¢ãƒ¼ãƒ‰) ---
st.set_page_config(layout="wide", page_title="æ²è¼‰çŠ¶æ³ç¢ºèªã‚¢ãƒ—ãƒª", page_icon="ğŸ“Š")

# --- ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ã®ä½™ç™½ã‚’ãªãã™CSSã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã«é–¢ã‚ã‚‰ãšè¡¨ç¤º ---
st.markdown("""
    <style>
            .block-container {
                padding-top: 1rem;
            }
            /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å¹…ã‚’600pxã«æŒ‡å®š */
            [data-testid="stSidebar"] {
                width: 600px !important;
            }
    </style>
""", unsafe_allow_html=True)

# CSSã®é©ç”¨
local_css("style.css")

# UIãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
st.title("ğŸ“Š æ²è¼‰çŠ¶æ³ç¢ºèªã‚¢ãƒ—ãƒª")
# --------------------------------------------------------------------

# --- Streamlit çµ„ã¿è¾¼ã¿èªè¨¼ ---

# 1. æœªãƒ­ã‚°ã‚¤ãƒ³ã®å ´åˆ: ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
if not st.user.get("is_logged_in", False):
    
    # ã‚«ãƒ©ãƒ ã‚’ä½¿ã£ã¦ä¸­å¤®ã«é…ç½®
    _, form_col, _ = st.columns([1.5, 1, 1.5])
    with form_col:
        # ä¸Šéƒ¨ã«30pxã®ä½™ç™½ã‚’è¿½åŠ 
        st.markdown("<div style='margin-top: 80px;'></div>", unsafe_allow_html=True)
        
        st.warning('Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚')
        
        # 1. ã¾ãšStreamlitã®ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹
        if st.button("Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³", icon=":material/login:", width='stretch'):
            # 2. ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰ã€st.login() ã‚’å‘¼ã³å‡ºã™ï¼ˆèªè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ï¼‰
            st.login() 
    
    # æœªãƒ­ã‚°ã‚¤ãƒ³æ™‚ã¯ã“ã“ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œã‚’åœæ­¢ã™ã‚‹
    st.stop()

# 2. ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã®å ´åˆ: ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚’è¡¨ç¤º
else:
    # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
    _, col2, col3 = st.columns([10, 3, 1.5], gap="small")
    with col2:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¯ st.user.name ã§å–å¾—å¯èƒ½
        st.markdown(f"<div style='text-align: right; margin-top: 22px;'>ã‚ˆã†ã“ã <b>{st.user.name}</b> ã•ã‚“</div>", unsafe_allow_html=True)
    with col3:
        st.markdown("<div style='margin-top: 0px;'>", unsafe_allow_html=True)
        if st.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", width='stretch'):
            st.logout()
        st.markdown("</div>", unsafe_allow_html=True)
    # ----------------------------------------------------

    # UIï¼ˆst.sidebarï¼‰ã‚ˆã‚Šå…ˆã«ã€å¿…è¦ãªé–¢æ•°ã‚„å¤‰æ•°ã‚’å®šç¾©ã™ã‚‹

    # --- Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š ---
    GSHEET_KEY = "1Yb-0DLDb-IAKIxDkhaSZxDl-zd2iDHZ3aX3_4mSiQyI"

    # --- Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºé–¢æ•° ---
    @st.cache_resource(ttl=600) # 10åˆ†é–“ service ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    def init_sheets_service():
        """Google Sheets API ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹"""
        try:
            # st.secretsã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
            google_credentials_info = json.loads(st.secrets["gcp_service_account"]["credentials_json"])
            
            # App 2 ã¨åŒã˜ 'readonly' ã‚¹ã‚³ãƒ¼ãƒ—ã‚’ä½¿ç”¨
            scopes = [
                'https://www.googleapis.com/auth/spreadsheets.readonly'
                # 'drive' ã‚¹ã‚³ãƒ¼ãƒ—ã¯ã‚·ãƒ¼ãƒˆã®èª­ã¿å–ã‚Šã ã‘ãªã‚‰ä¸è¦
            ]
            
            credentials = service_account.Credentials.from_service_account_info(
                google_credentials_info,
                scopes=scopes
            )
            
            service = build('sheets', 'v4', credentials=credentials)
            return service

        except KeyError:
            st.error("Googleèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`.streamlit/secrets.toml` ã« `[google]` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ `credentials_json` ã‚­ãƒ¼ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None
        except json.JSONDecodeError:
            st.error("Googleèªè¨¼æƒ…å ±ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚`.streamlit/secrets.toml` ã® `credentials_json` ãŒæœ‰åŠ¹ãªJSONæ–‡å­—åˆ—ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None
        except Exception as e:
            st.error(f"[init_sheets_service] Google Sheets ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.error("`.streamlit/secrets.toml` ã®è¨­å®šãŒæ­£ã—ã„ã‹ã€GCPã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒæœ‰åŠ¹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None
    
    def get_data_from_gsheet(_sheets_service, sheet_name, expected_headers):
        """æŒ‡å®šã•ã‚ŒãŸã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿DataFrameã¨ã—ã¦è¿”ã™ (googleapiclientç‰ˆ)"""
        try:
            # ã‚·ãƒ¼ãƒˆå…¨ä½“ (A1ã‹ã‚‰æœ€å¾Œã¾ã§) ã‚’å–å¾—ã™ã‚‹
            range_name = f"{sheet_name}!A1:ZZ" # å¿µã®ãŸã‚ZZåˆ—ã¾ã§
            
            result = _sheets_service.spreadsheets().values().get(
                spreadsheetId=GSHEET_KEY, 
                range=range_name
            ).execute()
            
            data = result.get('values', [])
            # â˜… gspread ã¨åŒã˜ 
            
            if not data:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚·ãƒ¼ãƒˆãŒç©ºã®ã‚ˆã†ã§ã™ï¼‰ã€‚")
                return pd.DataFrame(columns=expected_headers)

            headers = data[0]
            # å¿…è¦ãªåˆ—ã ã‘ã‚’æŠ½å‡º
            cols_to_use = []
            col_indices = []

            for header in expected_headers:
                try:
                    idx = headers.index(header)
                    cols_to_use.append(header)
                    col_indices.append(idx)
                except ValueError:
                    st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã«å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ '{header}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    return pd.DataFrame(columns=expected_headers)

            # 2è¡Œç›®ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            records = [
                tuple(row[i] for i in col_indices if i < len(row)) # â˜… è¡Œæœ«è¶…ãˆã‚¨ãƒ©ãƒ¼ã‚’é˜²æ­¢
                for row in data[1:]
            ]
            
            df = pd.DataFrame(records, columns=cols_to_use)
            return df

        except HttpError as err:
            # HttpError ã‚’ã‚­ãƒ£ãƒƒãƒ
            if err.resp.status == 404:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            elif err.resp.status == 403:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒå…±æœ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            else:
                st.error(f"[get_data_from_gsheet] ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®èª­ã¿è¾¼ã¿ä¸­ã« HttpError ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {err}")
            return pd.DataFrame(columns=expected_headers)
        except Exception as e:
            st.error(f"[get_data_from_gsheet] ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return pd.DataFrame(columns=expected_headers)

    # --- å„DBç”¨ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° (Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç‰ˆ) ---
    @st.cache_data(ttl=600) # 10åˆ†é–“ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    def get_teiki_data_from_gsheet(_sheets_service):
        """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰å®šæœŸä¾¿ç•ªå·ã®ã‚»ãƒƒãƒˆã‚’å–å¾—ã™ã‚‹"""
        if _sheets_service is None: return None
        
        sheet_name = "å®šæœŸä¾¿DB"
        expected_headers = ["å®šæœŸä¾¿ç•ªå·"]
        df = get_data_from_gsheet(_sheets_service, sheet_name, expected_headers)
        
        if df.empty:
            return set() # ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã™
            
        return set(df["å®šæœŸä¾¿ç•ªå·"].dropna().unique())

    @st.cache_data(ttl=600)
    def get_business_data_from_gsheet(_sheets_service):
        """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰äº‹æ¥­è€…ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã¨ã—ã¦å–å¾—ã™ã‚‹"""
        if _sheets_service is None: return pd.DataFrame(columns=["äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰", "äº‹æ¥­è€…å", "è‡ªæ²»ä½“å"])
        
        sheet_name = "äº‹æ¥­è€…DB"
        expected_headers = ["äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰", "äº‹æ¥­è€…å", "è‡ªæ²»ä½“å"]
        df = get_data_from_gsheet(_sheets_service, sheet_name, expected_headers)
        return df

    @st.cache_data(ttl=600)
    def get_product_data_from_gsheet(_sheets_service):
        """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰å•†å“ç®¡ç†ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã¨ã—ã¦å–å¾—ã™ã‚‹"""
        if _sheets_service is None: return pd.DataFrame(columns=["å•†å“ç•ªå·", "å•†å“ç®¡ç†ç•ªå·"])
        
        sheet_name = "å•†å“ç®¡ç†DB"
        expected_headers = ["å•†å“ç•ªå·", "å•†å“ç®¡ç†ç•ªå·"]
        df = get_data_from_gsheet(_sheets_service, sheet_name, expected_headers)
        return df

    # gspreadã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ– -> sheetsã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
    sheets_service = init_sheets_service()

    # è¿”ç¤¼å“ã€Œã‚³ãƒ¼ãƒ‰ã€åˆ—ã®å®šç¾©ï¼ˆãƒãƒ§ã‚¤ã‚¹ç³»ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã€ä»–ã¯ãƒ˜ãƒƒãƒ€ãƒ¼åï¼‰
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ãƒ˜ãƒƒãƒ€ãƒ¼åãƒªã‚¹ãƒˆã«åŸºã¥ãå®šç¾©
    KEY_COLUMN_MAP = {
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·)
        "ãƒãƒ§ã‚¤ã‚¹": 102,
        "ãƒãƒ§ã‚¤ã‚¹åœ¨åº«": 0,
        
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Š: ãƒ˜ãƒƒãƒ€ãƒ¼å)
        "æ¥½å¤©": "å•†å“ç•ªå·",
        "ANA": "è¿”ç¤¼å“è­˜åˆ¥ã‚³ãƒ¼ãƒ‰",
        "ãµã‚‹ãªã³": "å¤–éƒ¨è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 19)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "JAL": "è¿”ç¤¼å“ç•ªå·",
        "ã¾ã„ãµã‚‹": "è¿”ç¤¼å“ç•ªå·",
        "ãƒã‚¤ãƒŠãƒ“": "è¿”ç¤¼å“ç•ªå·",
        "ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ": "SKU", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 5)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "JRE": "å“ç•ª1", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 2)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "ã•ã¨ãµã‚‹": "ãŠç¤¼å“å", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 1, [code]æŠ½å‡º)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "ã•ã¨ãµã‚‹åœ¨åº«": "ãŠç¤¼å“ID", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 1)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "Amazon": "å‡ºå“è€…SKU" # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 0)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
    }

    # è¿”ç¤¼å“ã€Œåç§°ã€åˆ—ã®å®šç¾©ï¼ˆãƒãƒ§ã‚¤ã‚¹ç³»ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã€ä»–ã¯ãƒ˜ãƒƒãƒ€ãƒ¼åï¼‰
    PORTAL_NAME_COLUMN_MAP = {
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·)
        "ãƒãƒ§ã‚¤ã‚¹": 2,
        
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Š: ãƒ˜ãƒƒãƒ€ãƒ¼å)
        "æ¥½å¤©": "å•†å“å",
        "ANA": "è¿”ç¤¼å“å",
        "ãµã‚‹ãªã³": "è¿”ç¤¼å“å",
        "JAL": "è¿”ç¤¼å“å",
        "ã¾ã„ãµã‚‹": "è¿”ç¤¼å“å",
        "ãƒã‚¤ãƒŠãƒ“": "è¿”ç¤¼å“å",
        "ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ": "è¿”ç¤¼å“å",
        "JRE": "å•†å“å",
        "ã•ã¨ãµã‚‹": "ãŠç¤¼å“å", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 1)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "Amazon": None
    }

    PORTAL_ORDER = ['ãƒãƒ§ã‚¤ã‚¹', 'æ¥½å¤©', 'ANA', 'ãµã‚‹ãªã³', 'JAL', 'ã¾ã„ãµã‚‹', 'ãƒã‚¤ãƒŠãƒ“', 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ', 'JRE', 'ã•ã¨ãµã‚‹', 'Amazon']
    # TODAY_STR ã¯ L23 ã§å®šç¾©

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‚·ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
    SKIP_FILTERING_SHEETS = ['ãƒãƒ§ã‚¤ã‚¹åœ¨åº«', 'ã•ã¨ãµã‚‹åœ¨åº«']
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æŒãŸãªã„ï¼ˆ`header=None`ã§èª­ã¿è¾¼ã‚€ï¼‰ã‚·ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
    SHEETS_WITHOUT_HEADER = ['ãƒãƒ§ã‚¤ã‚¹', 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«']


    # --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
    def get_sheet_name_from_filename(filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚·ãƒ¼ãƒˆåã‚’æ¨æ¸¬ã™ã‚‹"""
        name_lower = filename.lower()
        if 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«' in name_lower: return 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«'
        if 'ãƒãƒ§ã‚¤ã‚¹' in name_lower: return 'ãƒãƒ§ã‚¤ã‚¹'
        if 'æ¥½å¤©' in name_lower: return 'æ¥½å¤©'
        if 'ana' in name_lower: return 'ANA'
        if 'ãµã‚‹ãªã³' in name_lower: return 'ãµã‚‹ãªã³'
        if 'jal' in name_lower: return 'JAL'
        if 'ã¾ã„ãµã‚‹' in name_lower: return 'ã¾ã„ãµã‚‹'
        if 'ãƒã‚¤ãƒŠãƒ“' in name_lower: return 'ãƒã‚¤ãƒŠãƒ“'
        if 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ' in name_lower: return 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ '
        if 'jre' in name_lower: return 'JRE'
        if 'ã•ã¨ãµã‚‹åœ¨åº«' in name_lower: return 'ã•ã¨ãµã‚‹åœ¨åº«'
        if 'ã•ã¨ãµã‚‹' in name_lower: return 'ã•ã¨ãµã‚‹'
        if 'amazon' in name_lower: return 'Amazon'
        return re.sub(r'\.(csv|tsv|xlsx)$', '', filename, flags=re.IGNORECASE)

    def robust_read_file(uploaded_file):
        """
        æ§˜ã€…ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨å½¢å¼ã«å¯¾å¿œã—ãŸãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°ã€‚
        ã‚·ãƒ¼ãƒˆåã«å¿œã˜ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã®æœ‰ç„¡ï¼ˆheader=0 or header=Noneï¼‰ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚
        """
        bytes_data = uploaded_file.getvalue()
        file_name = uploaded_file.name
        sheet_name = get_sheet_name_from_filename(file_name)
        
        # ã‚·ãƒ¼ãƒˆåã«åŸºã¥ã„ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã®æœ‰ç„¡ã‚’æ±ºå®š
        # ãƒãƒ§ã‚¤ã‚¹ç³»ã¯ header=Noneã€ãã‚Œä»¥å¤–ã¯ header=0 (1è¡Œç›®ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã™ã‚‹)
        header_setting = None if sheet_name in SHEETS_WITHOUT_HEADER else 0

        if file_name.endswith('.xlsx'):
            try:
                return pd.read_excel(BytesIO(bytes_data), header=header_setting, dtype=str).fillna('')
            except Exception as e:
                st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                return None

        separator = '\t' if file_name.lower().endswith('.tsv') else ','
        
        if 'æ¥½å¤©' in file_name.lower() or 'ã•ã¨ãµã‚‹' in file_name.lower():
            encodings_to_try = ['shift_jis', 'utf-8']
        elif any(n.lower() in file_name.lower() for n in ["N2", "ãƒãƒ§ã‚¤ã‚¹", "ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ", "amazon"]):
            encodings_to_try = ['utf-8', 'shift_jis']
        else:
            encodings_to_try = ['shift_jis', 'utf-8']

        for encoding in encodings_to_try:
            try:
                corrected_encoding = 'utf-8-sig' if encoding == 'utf-8' else encoding
                df = pd.read_csv(
                    BytesIO(bytes_data), 
                    header=header_setting,
                    encoding=corrected_encoding, 
                    dtype=str, 
                    sep=separator, 
                    engine='python', 
                    on_bad_lines='warn', 
                    encoding_errors='ignore'
                )
                return df.fillna('')
            except Exception:
                bytes_data = uploaded_file.getvalue()
                continue
                
        st.error(f"'{file_name}' ã‚’ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return None

    def generate_vendor_code(item_code):
        """è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‹ã‚‰äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹"""
        code = str(item_code).strip()
        if not code: return ''
        if re.match(r'^\d{2}[A-Z]{4}', code): return code[:6]
        if re.match(r'^[A-Z]{4}', code): return code[:4]
        if re.match(r'^[A-Z]{3}', code): return code[:3]
        return ''

    def filter_dataframe(df, sheet_name, item_codes_to_filter, vendor_codes_to_filter):
        """
        DataFrameã‚’æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹é–¢æ•°ã€‚
        KEY_COLUMN_MAPã®å€¤ãŒintã‹strã‹ã§å‡¦ç†ã‚’åˆ†å²ã™ã‚‹ã€‚
        """
        if df is None or df.empty:
            return df
        if not item_codes_to_filter and not vendor_codes_to_filter:
            return df
        
        key_col = KEY_COLUMN_MAP.get(sheet_name)
        if key_col is None:
            # ã‚­ãƒ¼åˆ—ãŒæœªå®šç¾©ãªã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãªã„
            return df
        
        data = df # robust_read_file ã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿ã®ãŸã‚ã€dfå…¨ä½“ãŒãƒ‡ãƒ¼ã‚¿
        
        if data.empty:
            return df # ãƒ‡ãƒ¼ã‚¿è¡ŒãŒãªã‘ã‚Œã°ãã®ã¾ã¾è¿”ã™
            
        item_code_series = pd.Series(dtype=str)
        
        # --- ã‚­ãƒ¼åˆ—ã®å‹ï¼ˆint or strï¼‰ã§å‡¦ç†ã‚’åˆ†å² ---
        if isinstance(key_col, int):
            # (ãƒãƒ§ã‚¤ã‚¹ç³»: ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã§å‚ç…§)
            if df.shape[1] <= key_col:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{sheet_name}' ã®åˆ—æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚­ãƒ¼åˆ— {key_col} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                return df
            
            # ãƒãƒ§ã‚¤ã‚¹ç³»ã¯ãã®ã¾ã¾ã®å€¤ã‚’ä½¿ç”¨
            item_code_series = data.iloc[:, key_col].astype(str).str.strip()

        elif isinstance(key_col, str):
            # (ãã®ä»–: ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šã€ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
            if key_col not in data.columns:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{sheet_name}' ã«å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ '{key_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return df
            
            # ã•ã¨ãµã‚‹ã®å ´åˆã€æ­£è¦è¡¨ç¾ã§ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            if sheet_name == 'ã•ã¨ãµã‚‹':
                # 'ãŠç¤¼å“å' åˆ—(key_col)ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º [xxxx] -> xxxx
                item_code_series = data[key_col].astype(str).str.extract(r'\[(.*?)\]', expand=False).fillna('')
            else:
                # ä»–ãƒãƒ¼ã‚¿ãƒ«ã¯ãã®ã¾ã¾ã®å€¤ã‚’ä½¿ç”¨
                item_code_series = data[key_col].astype(str).str.strip()
        
        else:
            # key_col ãŒ None ã¾ãŸã¯äºˆæœŸã›ã¬å‹
            return df

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ãƒã‚¹ã‚¯ã‚’åˆæœŸåŒ–
        mask = pd.Series(True, index=data.index)
        
        # 1. è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if item_codes_to_filter:
            # isinãƒ¡ã‚½ãƒƒãƒ‰ãŒæ©Ÿèƒ½ã™ã‚‹ã‚ˆã†ã«ã€ç©ºã®ã‚³ãƒ¼ãƒ‰ã‚’æŒã¤è¡Œã¯é™¤å¤–
            mask &= (item_code_series != '') & (item_code_series.isin(item_codes_to_filter))
        
        # 2. äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if vendor_codes_to_filter:
            # æŠ½å‡ºã¾ãŸã¯å–å¾—ã—ãŸè¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚·ãƒªãƒ¼ã‚ºã‹ã‚‰äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            vendor_code_series = item_code_series.apply(generate_vendor_code)
            # isinãƒ¡ã‚½ãƒƒãƒ‰ãŒæ©Ÿèƒ½ã™ã‚‹ã‚ˆã†ã«ã€ç©ºã®äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã‚’æŒã¤è¡Œã¯é™¤å¤–
            mask &= (vendor_code_series != '') & (vendor_code_series.isin(vendor_codes_to_filter))
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã¯ robust_read_file ã§å‡¦ç†æ¸ˆã¿ (df.columns ã«æ ¼ç´)
        # ãã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿è¡Œ (data) ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦è¿”ã™
        return data[mask]

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– (ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªç”¨)
    if 'uploader_key' not in st.session_state:
        st.session_state.uploader_key = 0
    if 'dataframes' not in st.session_state:
        st.session_state.dataframes = {}
    if 'results_df' not in st.session_state:
        st.session_state.results_df = pd.DataFrame()
    # (èªè¨¼é–¢é€£ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã¯StreamlitãŒå†…éƒ¨ã§ç®¡ç†ã™ã‚‹ãŸã‚ä¸è¦)

    # --- ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ ---
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 1
    # ----------------------------------------

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼UIã‚»ã‚¯ã‚·ãƒ§ãƒ³
    with st.sidebar:
        st.markdown('<h2 style="font-size: 24px;">1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†</h2>', unsafe_allow_html=True)

        with st.expander("DBï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰ã‚’é–‹ã"):
            st.markdown(
                "[ã€Œå®šæœŸä¾¿ç•ªå·ã€ã€Œäº‹æ¥­è€…ã€ã€Œå•†å“ç®¡ç†ç•ªå·ã€ã®ç™»éŒ²ã¯ã“ã¡ã‚‰ã‹ã‚‰](https://docs.google.com/spreadsheets/d/1Yb-0DLDb-IAKIxDkhaSZxDl-zd2iDHZ3aX3_4mSiQyI/)",
                unsafe_allow_html=True
            )
            st.info("ãƒ‡ãƒ¼ã‚¿ç·¨é›†å¾Œã¯ã€ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹• or ç”»é¢æ›´æ–°ï¼ˆã€ŒF5ã€ã‚­ãƒ¼ï¼‰ã‚’ã—ã¦ãã ã•ã„ã€‚")

        st.markdown('<h2 style="font-size: 24px;">2. ã‚¤ãƒ³ãƒãƒ¼ãƒˆ</h2>', unsafe_allow_html=True)

        with st.expander("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã‚’é–‹ã"):
            st.info("""
                ã“ã¡ã‚‰ã«å¯¾è±¡ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€èª­ã¿è¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚
                â€»ãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã€ã•ã¨ãµã‚‹åœ¨åº«ã¯ä»•æ§˜ä¸Šã€ã“ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å¯¾è±¡å¤–ã§ã™ã€‚
            """)
            filter_col1, filter_col2 = st.columns(2)
            with filter_col1:
                item_codes_to_filter_input = st.text_area(
                    "è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šã§å…¥åŠ›ï¼‰",
                    height=150,
                    key="filter_item_codes"
                )
            with filter_col2:
                vendor_codes_to_filter_input = st.text_area(
                    label="äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šã§å…¥åŠ›ï¼‰", # labelã‚’çŸ­ç¸®
                    height=150,
                    key="filter_vendor_codes",
                    help="â€»ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ3æ–‡å­— or ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ4æ–‡å­— or æ•°å­—2æ–‡å­—+ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ4æ–‡å­—ï¼ˆè¨ˆ6æ–‡å­—ï¼‰" # helpå¼•æ•°ã«èª¬æ˜ã‚’è¿½åŠ 
                )

        # --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ ---
        st.markdown('<h2 style="font-size: 18px;">< ãƒãƒ¼ã‚¿ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ></h2>', unsafe_allow_html=True)
        uploaded_files = st.file_uploader(
            "ãƒãƒ¼ã‚¿ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ (è¤‡æ•°é¸æŠå¯)",
            help="â€»ã€Œãƒãƒ§ã‚¤ã‚¹ã€ã¨ã€Œãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã€ã€ã€Œã•ã¨ãµã‚‹ã€ã¨ã€Œã•ã¨ãµã‚‹åœ¨åº«ã€ã¯ã€ãã‚Œãã‚Œå¿…ãšã‚»ãƒƒãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            type=['csv', 'tsv', 'xlsx'],
            accept_multiple_files=True,
            key=f"portal_uploader_{st.session_state.uploader_key}"
        )

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if uploaded_files:
            uploaded_file_names = {get_sheet_name_from_filename(f.name) for f in uploaded_files}

            # ãƒãƒ§ã‚¤ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢ãƒã‚§ãƒƒã‚¯
            is_choice_present = 'ãƒãƒ§ã‚¤ã‚¹' in uploaded_file_names
            is_choice_stock_present = 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«' in uploaded_file_names
            if is_choice_present ^ is_choice_stock_present:
                st.error("âš ï¸ ã€Œãƒãƒ§ã‚¤ã‚¹ã€ã¨ã€Œãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã€ã¯ã€å¿…ãšã‚»ãƒƒãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

            # ã•ã¨ãµã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢ãƒã‚§ãƒƒã‚¯
            is_sato_present = 'ã•ã¨ãµã‚‹' in uploaded_file_names
            is_sato_stock_present = 'ã•ã¨ãµã‚‹åœ¨åº«' in uploaded_file_names
            if is_sato_present ^ is_sato_stock_present:
                st.error("âš ï¸ ã€Œã•ã¨ãµã‚‹ã€ã¨ã€Œã•ã¨ãµã‚‹åœ¨åº«ã€ã¯ã€å¿…ãšã‚»ãƒƒãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

        def show_file_preview(uploaded_file, df_preview, num_rows=5):
            """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹"""
            with st.expander(f"ğŸ“„ **{uploaded_file.name}**"):
                st.dataframe(df_preview.head(num_rows))

        all_uploaded_files = uploaded_files or []

        files_to_process = []   # å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

        # --- ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã€ã™ãã«èª­ã¿è¾¼ã‚“ã§å‰å‡¦ç†ã‚’è¡Œã†
        if any(all_uploaded_files):

            # 1. ãƒãƒ¼ã‚¿ãƒ«åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ (äº‹å‰ãƒã‚§ãƒƒã‚¯)
            portal_to_file_map = {} # ã‚­ãƒ¼: sheet_name, å€¤: file.name
            files_to_reject = []    # æ‹’å¦å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ± (ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ)

            for file in all_uploaded_files:
                if file: # ãƒ•ã‚¡ã‚¤ãƒ«ãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    sheet_name = get_sheet_name_from_filename(file.name)
                    
                    if sheet_name in portal_to_file_map:
                        # é‡è¤‡æ¤œå‡º: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ‹’å¦ãƒªã‚¹ãƒˆã¸
                        original_file_name = portal_to_file_map[sheet_name]
                        files_to_reject.append((file.name, sheet_name, original_file_name))
                    else:
                        # æ–°è¦ãƒãƒ¼ã‚¿ãƒ«: å‡¦ç†å¯¾è±¡ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        portal_to_file_map[sheet_name] = file.name
                        files_to_process.append(file)

            # 2. æ‹’å¦ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            if files_to_reject:
                for file_name, portal_name, original_file_name in files_to_reject:
                    st.error(f"âš ï¸ **{file_name}** ã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚**'{portal_name}'** ãƒãƒ¼ã‚¿ãƒ«ã¯æ—¢ã« **{original_file_name}** ã«ã‚ˆã£ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚")

            # â˜… ãƒˆãƒ¼ã‚¹ãƒˆè¡¨ç¤ºç”¨ã®ãƒ•ãƒ©ã‚°ã‚’åˆæœŸåŒ–
            new_file_processed = False

            item_codes_list = [code.strip() for code in item_codes_to_filter_input.split('\n') if code.strip()]
            vendor_codes_list = [code.strip() for code in vendor_codes_to_filter_input.split('\n') if code.strip()]

            # 3. å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ« (files_to_process) ã®ã¿ãƒ«ãƒ¼ãƒ—å‡¦ç†
            for file in files_to_process:
            
                # fileãŒNoneã®å¯èƒ½æ€§ã¯äº‹å‰ãƒã‚§ãƒƒã‚¯ã§æ’é™¤ã•ã‚Œã¦ã„ã‚‹
                sheet_name = get_sheet_name_from_filename(file.name)
                
                # â˜… å¤‰æ›´: file_id ã®ä»£ã‚ã‚Šã«ã€åå‰ã€ã‚µã‚¤ã‚ºã€ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ„æ€§ã‚’åˆ¤æ–­
                file_key = f"{sheet_name}_metadata"
                current_metadata = (file.name, file.size, file.type)

                # æ—¢ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã£ã¦ã„ãªã„å ´åˆã¯å†èª­ã¿è¾¼ã¿ã—ãªã„
                # â˜… å¤‰æ›´: file_id ã®æ¯”è¼ƒã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ¯”è¼ƒã«å¤‰æ›´
                if sheet_name not in st.session_state.dataframes or st.session_state.dataframes.get(file_key) != current_metadata:
                    df = robust_read_file(file)
                    if df is not None:
                        if sheet_name not in SKIP_FILTERING_SHEETS:
                            df = filter_dataframe(df, sheet_name, item_codes_list, vendor_codes_list)
                        st.session_state.dataframes[sheet_name] = df
                        st.session_state.dataframes[file_key] = current_metadata # â˜… å¤‰æ›´: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜

                        new_file_processed = True # â˜… æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹

                        # å‰å‡¦ç†ãƒ•ãƒ©ã‚°ã®ãƒªã‚»ãƒƒãƒˆ
                        if sheet_name == 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«': st.session_state['choice_stock_processed'] = False

            # --- ãƒãƒ§ã‚¤ã‚¹åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç† ---
            # ãƒãƒ§ã‚¤ã‚¹ã¨ãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã®ä¸¡æ–¹ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¦ã€ã¾ã å‰å‡¦ç†ãŒã•ã‚Œã¦ã„ãªã„å ´åˆ
            if "ãƒãƒ§ã‚¤ã‚¹" in st.session_state.dataframes and "ãƒãƒ§ã‚¤ã‚¹åœ¨åº«" in st.session_state.dataframes:
                if not st.session_state.get('choice_stock_processed', False):
                    df_choice = st.session_state.dataframes["ãƒãƒ§ã‚¤ã‚¹"]
                    df_choice_stock = st.session_state.dataframes["ãƒãƒ§ã‚¤ã‚¹åœ¨åº«"].copy()
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    # (ãƒãƒ§ã‚¤ã‚¹: index 1, 102), (ãƒãƒ§ã‚¤ã‚¹åœ¨åº«: index 1)
                    if df_choice.shape[1] > 102 and df_choice_stock.shape[1] > 1:
                        # ãƒãƒ§ã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ å•†å“ç®¡ç†ç•ªå·(1) ã¨ è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰(102) ã‚’æŠ½å‡º
                        df_map_source = df_choice[[1, 102]].dropna().copy()
                        # æ–‡å­—åˆ—å‹ã«ã—ã¦å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
                        df_map_source[1] = df_map_source[1].astype(str).str.strip()
                        df_map_source[102] = df_map_source[102].astype(str).str.strip()
                        # å•†å“ç®¡ç†ç•ªå·ã§é‡è¤‡ã‚’é™¤å» (æœ€åˆã®ä¸€ã¤ã‚’æ®‹ã™)
                        df_map_source = df_map_source.drop_duplicates(subset=[1], keep='first')
                        # å•†å“ç®¡ç†ç•ªå·ã‚’ã‚­ãƒ¼ã€è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
                        id_map = df_map_source.set_index(1)[102].to_dict()
                        # ãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã® å•†å“ç®¡ç†ç•ªå·(1) åˆ—ã‚’å–å¾—ã—ã€æ–‡å­—åˆ—å‹ã«å¤‰æ›
                        lookup_keys = df_choice_stock[1].astype(str).str.strip()
                        # mapé–¢æ•°ã‚’ä½¿ã£ã¦è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚’ç´ä»˜ã‘
                        mapped_codes = lookup_keys.map(id_map)
                        # ç´ä»˜ã‘ãŸè¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚’å…ˆé ­åˆ—(0åˆ—ç›®)ã«æŒ¿å…¥
                        df_choice_stock.insert(0, 'generated_code', mapped_codes)
                        # åˆ—åã‚’ãƒªã‚»ãƒƒãƒˆ (0, 1, 2, ...)
                        df_choice_stock.columns = range(df_choice_stock.shape[1])
                        # å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                        st.session_state.dataframes["ãƒãƒ§ã‚¤ã‚¹åœ¨åº«"] = df_choice_stock
                        # å‰å‡¦ç†æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                        st.session_state['choice_stock_processed'] = True

                        # â˜… åœ¨åº«å‡¦ç†ã‚‚æ–°è¦å‡¦ç†ã¨ã¿ãªã™
                        if not new_file_processed: # é‡è¤‡ãƒˆãƒ¼ã‚¹ãƒˆã‚’é¿ã‘ã‚‹
                            new_file_processed = True

            # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãŒå®Œäº†ã—ãŸå ´åˆã«ãƒˆãƒ¼ã‚¹ãƒˆã‚’è¡¨ç¤º
            if new_file_processed:
                st.toast("ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚", icon="ğŸ“„")

        # --- ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ Expander ---
        # session_state.dataframes ã«ãƒ•ã‚¡ã‚¤ãƒ«IDä»¥å¤–ã®ã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        # â˜… å¤‰æ›´: _id -> _metadata
        processed_dataframes_exist = any(not k.endswith('_metadata') for k in st.session_state.dataframes)

        # å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ Expander ã‚’è¡¨ç¤º
        if processed_dataframes_exist:
            with st.expander("ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                col_idx = 0
                processed_files_count = 0 # æ­£å¸¸ã«å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                for file in files_to_process:
                    if file:
                        sheet_name = get_sheet_name_from_filename(file.name)
                        # å‰å‡¦ç†ã®çµæœã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                        if sheet_name in st.session_state.dataframes:
                            show_file_preview(file, st.session_state.dataframes[sheet_name])
                            processed_files_count += 1
                # ã‚‚ã—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚‹ã®ã«ã€å‡¦ç†ã•ã‚ŒãŸã‚‚ã®ãŒãªã‘ã‚Œã°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
                if processed_files_count == 0 and any(all_uploaded_files):
                    st.write("ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿/å‡¦ç†ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ãã¾ã›ã‚“ã€‚")
                elif processed_files_count == 0: # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãªã„å ´åˆ
                    st.write("ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«é¸æŠæ©Ÿèƒ½
        st.markdown('<h2 style="font-size: 18px;">< ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ãƒ»åŸºæº–æ—¥ã®è¨­å®š ></h2>', unsafe_allow_html=True)
        st.markdown('<p style="font-size: 14px; margin-top: -10px; margin-left: 20px;">é¸æŠã•ã‚ŒãŸãƒãƒ¼ã‚¿ãƒ«ã¨åŸºæº–æ—¥ã‚’å…ƒã«æ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚</p>', unsafe_allow_html=True)

        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒãƒ¼ã‚¿ãƒ«åã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        # â˜… å¤‰æ›´: _id -> _metadata
        uploaded_portal_names = [p for p in PORTAL_ORDER if p in st.session_state.dataframes and not p.endswith('_metadata')]

        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
        files_uploaded = bool(uploaded_portal_names)
        
        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã¨æ—¥ä»˜é¸æŠã‚’ã‚«ãƒ©ãƒ ã§æ¨ªä¸¦ã³ã«ã™ã‚‹
        col1, col2 = st.columns([2, 1]) # 2:1 ã®æ¯”ç‡

        with col1:
            if files_uploaded:
                # ã€Œãƒãƒ§ã‚¤ã‚¹ã€ãŒã‚ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
                default_index = 0
                if "ãƒãƒ§ã‚¤ã‚¹" in uploaded_portal_names:
                    default_index = uploaded_portal_names.index("ãƒãƒ§ã‚¤ã‚¹")

                selected_base_portal = st.selectbox(
                    label="ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«é¸æŠ",
                    options=uploaded_portal_names,
                    index=default_index, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠã‚’è¨­å®š
                    label_visibility="collapsed"
                )
            else:
                selected_base_portal = None
                st.selectbox(
                    label="ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«é¸æŠ",
                    options=["ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"],
                    disabled=True,
                    label_visibility="collapsed"
                )

        # æ—¥ä»˜é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ (ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼)
        with col2:
            selected_date = st.date_input(
                label="åŸºæº–æ—¥", # ãƒ©ãƒ™ãƒ«ã¯éè¡¨ç¤º
                value=TODAY, # L22 ã§å®šç¾©ã—ãŸæœ¬æ—¥æ—¥ä»˜
                disabled=not files_uploaded,
                label_visibility="collapsed",
                help="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šã®åŸºæº–ã¨ãªã‚‹æ—¥ä»˜ã‚’é¸æŠã—ã¾ã™ã€‚"
            )

        # --- ã€Œæ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤ºã€ãƒœã‚¿ãƒ³ ---
        st.markdown('<div class="button-container" style="margin-top: 10px;">', unsafe_allow_html=True)
        run_button = st.button(
            "æ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤º",
            key="sidebar_run_button",
            disabled=not files_uploaded # ãƒªã‚¹ãƒˆãŒç©ºãªã‚‰True (éã‚¢ã‚¯ãƒ†ã‚£ãƒ–)
        )
        st.markdown('</div>', unsafe_allow_html=True)


    # --- ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸UIã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
    if run_button:
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ­£å¸¸ã‹ãƒã‚§ãƒƒã‚¯
        if sheets_service is None:
            st.error("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚èªè¨¼è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()
            
        # å‡¦ç†å®Ÿè¡Œå‰ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
        # â˜… å¤‰æ›´: _id -> _metadata
        loaded_df_names = {k for k in st.session_state.dataframes if not k.endswith('_metadata')}
        
        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹
        if selected_base_portal is None:
            st.error("ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‚’é¸æŠã§ãã¾ã›ã‚“ã€‚")

        # ã•ã¨ãµã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        is_sato_present = 'ã•ã¨ãµã‚‹' in loaded_df_names
        is_sato_stock_present = 'ã•ã¨ãµã‚‹åœ¨åº«' in loaded_df_names
        satofuru_files_ok = not (is_sato_present ^ is_sato_stock_present)
        if not satofuru_files_ok:
            st.error("ã€Œã•ã¨ãµã‚‹ã€ã¨ã€Œã•ã¨ãµã‚‹åœ¨åº«ã€ã¯ä¸¡æ–¹åŒæ™‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ãŒNoneã§ãªã„ã“ã¨ã¨ã€ã•ã¨ãµã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒOKãªã“ã¨ã‚’ç¢ºèª
        if selected_base_portal and satofuru_files_ok:
            
            # é¸æŠã•ã‚ŒãŸæ—¥ä»˜ã‚’ 'YYYYMMDD' å½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
            select_date_str = selected_date.strftime('%Y%m%d')
            
            # â˜… è¿½åŠ : ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«åŸºæº–æ—¥ã¨ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‚’ä¿å­˜
            st.session_state.current_select_date_str = select_date_str
            st.session_state.current_base_portal = selected_base_portal

            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨ˆç®—ä¸­..."):
                try:
                    teiki_bin_codes = get_teiki_data_from_gsheet(sheets_service)
                    if teiki_bin_codes is None: # å–å¾—å¤±æ•—
                        st.error("å®šæœŸä¾¿DBï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.stop()
                    
                    df_product_db = get_product_data_from_gsheet(sheets_service)
                    if df_product_db is None: # å–å¾—å¤±æ•—
                        st.error("å•†å“ç®¡ç†DBï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.stop()
                    
                    df_business = get_business_data_from_gsheet(sheets_service)
                    if df_business is None: # å–å¾—å¤±æ•—
                        st.error("äº‹æ¥­è€…DBï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.stop()
                    # ---------------------------------

                    # â˜… å¤‰æ›´: _id -> _metadata
                    full_data = {k: v for k, v in st.session_state.dataframes.items() if not k.endswith('_metadata')}
                    
                    master_items = {}
                    base_portal_name = selected_base_portal
                    df_base = full_data.get(base_portal_name)
                    
                    # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã¨åç§°ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                    df_base_data = df_base # robust_read_fileã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿
                    
                    if df_base_data is not None:
                        code_col = KEY_COLUMN_MAP.get(base_portal_name)
                        name_col = PORTAL_NAME_COLUMN_MAP.get(base_portal_name)

                        if code_col is not None:
                            # gspread ã¨ googleapiclient ã§ .dropna() ã®æŒ™å‹•ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚
                            # ã‚­ãƒ¼åˆ—ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ subset ã‚’æŒ‡å®šã™ã‚‹
                            subset_col = [code_col] if code_col in df_base_data.columns or isinstance(code_col, int) else None
                            if subset_col:
                                df_master_source = df_base_data.dropna(subset=subset_col).copy()
                            else:
                                df_master_source = df_base_data.copy() # subset ãªã— (ä¸‡ãŒä¸€ã®å ´åˆ)

                            
                            # --- ã‚­ãƒ¼åˆ—ã®å‹ï¼ˆint or strï¼‰ã§å‡¦ç†ã‚’åˆ†å² ---
                            if isinstance(code_col, int):
                                # (ãƒãƒ§ã‚¤ã‚¹ç³»: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã§å‚ç…§)
                                # (lookup_mapså´ã¨ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã‚’åˆã‚ã›ã‚‹)
                                # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                df_master_source['key'] = df_master_source[code_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                            
                            elif isinstance(code_col, str):
                                # (ãã®ä»–: ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
                                if base_portal_name == 'ã•ã¨ãµã‚‹':
                                    # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                    df_master_source['key'] = df_master_source[code_col].astype(str).str.extract(r'\[(.*?)\]', expand=False).fillna('').str.upper()
                                else:
                                    # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                    df_master_source['key'] = df_master_source[code_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                            # é‡è¤‡ã‚’é™¤å»
                            unique_items = df_master_source[df_master_source['key'] != ''].drop_duplicates(subset=['key'], keep='first')

                            # ãƒã‚¹ã‚¿ãƒ¼è¾æ›¸ã‚’ä½œæˆ
                            for _, row in unique_items.iterrows():
                                item_code = row['key']
                                item_name = ""
                                if name_col is not None:
                                    try:
                                        item_name = str(row[name_col]).strip()
                                    except KeyError:
                                        item_name = ""
                                master_items[item_code] = item_name
                        
                    lookup_maps, parent_lookup_maps = {}, {}

                    # --- æ¥½å¤©ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---
                    # Håˆ—(å•†å“ç•ªå·) -> Gåˆ—(å•†å“ç®¡ç†ç•ªå·) ã®å¯¾å¿œè¾æ›¸ã‚’ä½œæˆ
                    # â˜… å¤‰æ›´: GSheetã®ã€Œå•†å“ç•ªå·ã€ã‚‚ .upper() ã«çµ±ä¸€
                    df_product_db_copy = df_product_db.copy()
                    df_product_db_copy['å•†å“ç•ªå·_upper'] = df_product_db_copy['å•†å“ç•ªå·'].astype(str).str.strip().str.upper()
                    # é‡è¤‡ã‚’é™¤å»
                    df_product_db_copy = df_product_db_copy.dropna(subset=['å•†å“ç•ªå·_upper', 'å•†å“ç®¡ç†ç•ªå·'])
                    df_product_db_copy = df_product_db_copy.drop_duplicates(subset=['å•†å“ç•ªå·_upper'], keep='first')
            
                    memo_map = pd.Series(
                        df_product_db_copy['å•†å“ç®¡ç†ç•ªå·'].values, 
                        index=df_product_db_copy['å•†å“ç•ªå·_upper']
                    ).to_dict()

                    # æ¥½å¤©ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ç¨®å¯¾å¿œè¾æ›¸ã‚’ä½œæˆ (ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
                    rakuten_product_id_map = {} # å•†å“ç•ªå· -> è¡Œãƒ‡ãƒ¼ã‚¿
                    rakuten_management_id_map = {} # å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰ -> è¡Œãƒ‡ãƒ¼ã‚¿
                    rakuten_sku_code_map = {} # SKUç®¡ç†ç•ªå· -> è¡Œãƒ‡ãƒ¼ã‚¿

                    if 'æ¥½å¤©' in full_data:
                        df_rakuten = full_data['æ¥½å¤©']
                        # robust_read_fileã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿ã®ãŸã‚ã€iloc[1:] ã¯ä¸è¦
                        df_rakuten_data = df_rakuten
                        
                        # Båˆ—(å•†å“ç•ªå·) -> è¡Œãƒ‡ãƒ¼ã‚¿
                        if 'å•†å“ç•ªå·' in df_rakuten_data.columns:
                            df_rakuten_b = df_rakuten_data.dropna(subset=['å•†å“ç•ªå·']).drop_duplicates(subset=['å•†å“ç•ªå·'], keep='first')
                            # â˜… å¤‰æ›´: .upper() ã«çµ±ä¸€
                            rakuten_product_id_map = {str(row['å•†å“ç•ªå·']).strip().upper(): row.to_dict() for _, row in df_rakuten_b.iterrows()}
                        
                        # Aåˆ—(å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰) -> è¡Œãƒ‡ãƒ¼ã‚¿
                        if 'å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰' in df_rakuten_data.columns:
                            df_rakuten_a = df_rakuten_data.dropna(subset=['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰']).drop_duplicates(subset=['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰'], keep='first')
                            # â˜… å¤‰æ›´: .upper() ã«çµ±ä¸€
                            rakuten_management_id_map = {str(row['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰']).strip().upper(): row.to_dict() for _, row in df_rakuten_a.iterrows()}

                        # Håˆ—(SKUç®¡ç†ç•ªå·) -> è¡Œãƒ‡ãƒ¼ã‚¿
                        if 'SKUç®¡ç†ç•ªå·' in df_rakuten_data.columns:
                            df_rakuten_h = df_rakuten_data.dropna(subset=['SKUç®¡ç†ç•ªå·']).drop_duplicates(subset=['SKUç®¡ç†ç•ªå·'], keep='first')
                            # â˜… é‡è¦: SKUç®¡ç†ç•ªå·ã¯å³å¯†æ¯”è¼ƒã®ãŸã‚ã€.upper() ã—ãªã„ (å…ƒã®ã¾ã¾)
                            rakuten_sku_code_map = {str(row['SKUç®¡ç†ç•ªå·']).strip(): row.to_dict() for _, row in df_rakuten_h.iterrows()}
                    
                    # --- ä»–ãƒãƒ¼ã‚¿ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ (lookup_maps ä½œæˆ) ---
                    for name, df in full_data.items():
                        key_col = KEY_COLUMN_MAP.get(name)
                        
                        if key_col is None:
                            continue # ã‚­ãƒ¼åˆ—ãŒæœªå®šç¾©ã®ã‚·ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        
                        df_data_only = df # robust_read_fileã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿
                        
                        # --- ã‚­ãƒ¼åˆ—ã®å‹ï¼ˆint or strï¼‰ã§å‡¦ç†ã‚’åˆ†å² ---
                        if isinstance(key_col, int):
                            # (ãƒãƒ§ã‚¤ã‚¹ç³»: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã§å‚ç…§)
                            if df.shape[1] <= key_col:
                                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{name}' ã®åˆ—æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚­ãƒ¼åˆ— {key_col} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                                continue
                                
                            df_cleaned = df_data_only.dropna(subset=[key_col]).copy()
                            # BOMç­‰ã®é™¤å»ã€.0é™¤å»ã€ç©ºç™½é™¤å»
                            # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                            df_cleaned['key_col_str'] = df_cleaned[key_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                            df_cleaned = df_cleaned[df_cleaned['key_col_str'] != '']
                            
                            unique_data = df_cleaned.drop_duplicates(subset=['key_col_str'], keep='first')
                            # ã‚­ãƒ¼ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·(0, 1...)ã®è¾æ›¸ã‚’ä½œæˆ
                            lookup_maps[name] = {row['key_col_str']: row.to_dict() for _, row in unique_data.iterrows()}

                        elif isinstance(key_col, str):
                            # (ãã®ä»–: ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
                            if key_col not in df.columns:
                                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{name}' ã«å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ '{key_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                                continue
                            
                            df_cleaned = df_data_only.dropna(subset=[key_col]).copy()
                            
                            if name == 'ã•ã¨ãµã‚‹':
                                temp_map = {}
                                for _, row in df_cleaned.iterrows():
                                    # 'ãŠç¤¼å“å' åˆ—(key_col)ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                                    match = re.search(r'\[(.*?)\]', str(row.get(key_col, ''))) 
                                    if match:
                                        # â˜… å¤‰æ›´: ã™ã¹ã¦ .upper() ã«çµ±ä¸€
                                        key = match.group(1).strip().upper()
                                        if key and key not in temp_map:
                                            # ã‚­ãƒ¼ãŒãƒ˜ãƒƒãƒ€ãƒ¼å('ãŠç¤¼å“ID', 'ãŠç¤¼å“å'...)ã®è¾æ›¸ã‚’ä½œæˆ
                                            temp_map[key] = row.to_dict()
                                lookup_maps[name] = temp_map
                            else:
                                # BOMç­‰ã®é™¤å»ã€.0é™¤å»ã€ç©ºç™½é™¤å»
                                # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                df_cleaned['key_col_str'] = df_cleaned[key_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                                df_cleaned = df_cleaned[df_cleaned['key_col_str'] != '']
                                
                                unique_data = df_cleaned.drop_duplicates(subset=['key_col_str'], keep='first')
                                # ã‚­ãƒ¼ãŒãƒ˜ãƒƒãƒ€ãƒ¼å('å•†å“ç•ªå·', 'å•†å“å'...)ã®è¾æ›¸ã‚’ä½œæˆ
                                lookup_maps[name] = {row['key_col_str']: row.to_dict() for _, row in unique_data.iterrows()}

                    results_data = []
                    uploaded_portals = [p for p in PORTAL_ORDER if p in full_data]

                    for code, name in master_items.items():
                        statuses = {
                            portal: calculate_status(
                                portal, code, lookup_maps, parent_lookup_maps,
                                
                                # åŸºæº–æ—¥(æ–‡å­—åˆ—)ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã¨ã—ã¦æ¸¡ã™
                                select_date_str=select_date_str,
                                
                                # æ¥½å¤©ç”¨ã®è¾æ›¸ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã¨ã—ã¦æ¸¡ã™
                                memo_map=memo_map,
                                rakuten_product_id_map=rakuten_product_id_map,
                                rakuten_management_id_map=rakuten_management_id_map,
                                rakuten_sku_code_map=rakuten_sku_code_map
                            ) for portal in uploaded_portals
                        }
                        
                        status_values = list(statuses.values())
                        check_val = "OK" if len(set(status_values)) <= 1 else "è¦ç¢ºèª"
                        public_count = sum(1 for s in status_values if s == 'å…¬é–‹ä¸­')
                        
                        teiki_bin_flag = 'ã€‡' if code in teiki_bin_codes else 'Ã—'
                            
                        result_row = {'è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰': code, 'è¿”ç¤¼å“å': name, 'äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰': generate_vendor_code(code), **statuses,
                                    'ãƒã‚§ãƒƒã‚¯': check_val, 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°': teiki_bin_flag, 'å…¬é–‹ä¸­ã®æ•°': public_count}
                        results_data.append(result_row)
                    
                    if results_data:
                        df_results = pd.DataFrame(results_data)
                        
                        # df_business ã¯ Gsheetã‹ã‚‰å–å¾—æ¸ˆã¿ã®ã‚‚ã®ã‚’ä½¿ç”¨
                        if not df_business.empty:
                            df_business_names = df_business[['äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­è€…å']]
                            df_results = pd.merge(df_results, df_business_names, on='äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰', how='left')
                            df_results['äº‹æ¥­è€…å'] = df_results['äº‹æ¥­è€…å'].fillna('')
                        else:
                            df_results['äº‹æ¥­è€…å'] = ''
                        
                        base_columns = ['è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰', 'è¿”ç¤¼å“å', 'äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­è€…å']
                        base_portal_column_list = [base_portal_name] if base_portal_name in df_results.columns else []
                        other_portal_columns = [
                            p for p in PORTAL_ORDER 
                            if p in df_results.columns and p != base_portal_name
                        ]
                        utility_columns = ['ãƒã‚§ãƒƒã‚¯', 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°', 'å…¬é–‹ä¸­ã®æ•°']
                        display_columns = base_columns + base_portal_column_list + other_portal_columns + utility_columns
                        final_display_columns = [col for col in display_columns if col in df_results.columns]
                        st.session_state.results_df = df_results.reindex(columns=final_display_columns)
                    else:
                        st.session_state.results_df = pd.DataFrame()
                    
                except Exception as e:
                    st.error(f"å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); import traceback; st.code(traceback.format_exc())
                    st.session_state.results_df = pd.DataFrame()

        # â˜… è¿½åŠ : å‡¦ç†å®Œäº†ã®ãƒˆãƒ¼ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        st.toast("æ²è¼‰çŠ¶æ³ã®è¡¨ç¤ºã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚", icon="ğŸ“Š")

    st.markdown('<h2 style="font-size: 26px;">3. æ²è¼‰çŠ¶æ³</h2>', unsafe_allow_html=True)

    # ãƒªã‚»ãƒƒãƒˆå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    if 'show_reset_success' in st.session_state:
        st.toast("æ²è¼‰çŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚", icon="âœ…")
        del st.session_state.show_reset_success

    if st.session_state.results_df.empty:
        if run_button:
            st.warning("è¡¨ç¤ºå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚„DBã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰è¨­å®šã—ã€ã€Œæ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤ºã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    else:
        df_to_display = st.session_state.results_df.copy()
        
        # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        filter_cols = st.columns(4)

        with filter_cols[0]:
            search_text = st.text_input("å…¨æ–‡æ¤œç´¢ (ã‚³ãƒ¼ãƒ‰/è¿”ç¤¼å“å/äº‹æ¥­è€…å):")
            if search_text:
                df_to_display = df_to_display[
                    df_to_display['è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰'].str.contains(search_text, na=False, case=False) |
                    df_to_display['è¿”ç¤¼å“å'].str.contains(search_text, na=False, case=False) |
                    df_to_display['äº‹æ¥­è€…å'].str.contains(search_text, na=False, case=False)
                ]

        with filter_cols[1]:
            vendor_list = sorted(st.session_state.results_df['äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰'].unique())
            vendor_options = ["ã™ã¹ã¦"] + vendor_list
            selected_vendor = st.selectbox("äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰:", vendor_options)
            if selected_vendor != "ã™ã¹ã¦":
                df_to_display = df_to_display[df_to_display['äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰'] == selected_vendor]

        with filter_cols[2]:
            check_options = ["ã™ã¹ã¦", "OK", "è¦ç¢ºèª"]
            selected_check = st.selectbox("ãƒã‚§ãƒƒã‚¯:", check_options)
            if selected_check != "ã™ã¹ã¦":
                df_to_display = df_to_display[df_to_display['ãƒã‚§ãƒƒã‚¯'] == selected_check]

        with filter_cols[3]:
            teiki_options = ["ã™ã¹ã¦", "ã€‡", "Ã—"]
            selected_teiki = st.selectbox("å®šæœŸä¾¿:", teiki_options)
            if selected_teiki != "ã™ã¹ã¦":
                df_to_display = df_to_display[df_to_display['å®šæœŸä¾¿ãƒ•ãƒ©ã‚°'] == selected_teiki]
        
        # --- ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š (â˜… DataFrameæç”»å‰ã«è¨ˆç®—å‡¦ç†ã‚’ç§»å‹• â˜…) ---
        # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®è¡¨ç¤ºä»¶æ•°
        ITEMS_PER_PAGE = 500 
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’è¨ˆç®—
        total_items = len(df_to_display)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã«åŸºã¥ãã€ç·ãƒšãƒ¼ã‚¸æ•°ã¨ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’è¨ˆç®—ãƒ»è£œæ­£
        if total_items > 0:
            # ç·ãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—
            total_pages = (total_items // ITEMS_PER_PAGE) + (1 if total_items % ITEMS_PER_PAGE > 0 else 0)
            
            # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å–å¾—
            current_page = st.session_state.current_page
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨å¾Œã« total_pages ãŒæ¸›ã£ãŸå ´åˆã€ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ãŒæœ€å¤§ãƒšãƒ¼ã‚¸ã‚’è¶…ãˆãªã„ã‚ˆã†ã«è£œæ­£
            if current_page > total_pages:
                st.session_state.current_page = total_pages
                current_page = total_pages # ã‚¹ãƒ©ã‚¤ã‚¹å‡¦ç†ç”¨ã«ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‚‚æ›´æ–°
        else:
            # ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®å ´åˆ
            total_pages = 1
            st.session_state.current_page = 1
            current_page = 1
        
        st.write("")

        # è¡¨ç¤ºä»¶æ•°ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ã‚’æ¨ªä¸¦ã³ã«é…ç½®
        count_col, _, button_col = st.columns([3, 6, 4]) 

        with count_col:
            # èª­ã¿å–ã£ãŸã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã®ä»¶æ•°
            total_count = len(st.session_state.results_df)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ã‹ã‘ãŸçŠ¶æ…‹ã®ä»¶æ•°
            filtered_count = len(df_to_display)
            
            # --- è¡¨ç¤ºå½¢å¼ã‚’åˆ†å² ---
            if total_count == filtered_count:
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒã‹ã‹ã£ã¦ã„ãªã„å ´åˆ (ã¾ãŸã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç·ä»¶æ•°ã¨ä¸€è‡´ã™ã‚‹å ´åˆ)
                display_text = f"{total_count}ä»¶ è¡¨ç¤º"
            else:
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒã‹ã‹ã£ã¦ã„ã‚‹å ´åˆ
                display_text = f"{filtered_count} / {total_count}ä»¶ è¡¨ç¤º"

            # â˜… HTML/CSSã§ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’å¤§ãã (1.1rem) ã—ã¦è¡¨ç¤º
            st.markdown(
                f"""
                <span style='font-size: 1.1rem; font-weight: bold; white-space: nowrap;'>
                {display_text}
                </span>
                """, 
                unsafe_allow_html=True
            )

        EXCEL_COLOR_MAP = {
            # 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': (bg_color, font_color)
            'å…¬é–‹ä¸­': ('#22a579', '#FFFFFF'),
            'æœªç™»éŒ²': ('#111111', '#FFFFFF'),
            'å—ä»˜çµ‚äº†': ('#6c757d', '#FFFFFF'),
            'éè¡¨ç¤º': ('#6c757d', '#FFFFFF'),
            'åœ¨åº«0': ('#6c757d', '#FFFFFF'),
            'å€‰åº«': ('#6c757d', '#FFFFFF'),
            'æœªå—ä»˜': ('#ffc107', '#000000'), # æœªå—ä»˜ã¯é»’æ–‡å­—
            'è¦ç¢ºèª': ('#fa6c78', '#000000')  # è¦ç¢ºèªã¯é»’æ–‡å­—
        }

        # --- to_excel é–¢æ•° ---
        def to_excel(df):
            output = BytesIO()
            # XlsxWriter ã‚’ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦æŒ‡å®š
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                sheet_name = 'Sheet1'
                workbook = writer.book
                
                # --- 1. æ›¸å¼(ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)ã®å®šç¾© ---
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ›¸å¼ (ãƒ•ã‚©ãƒ³ãƒˆ: æ¸¸ã‚´ã‚·ãƒƒã‚¯)
                default_format = workbook.add_format({
                    'font_name': 'æ¸¸ã‚´ã‚·ãƒƒã‚¯'
                })

                # ãƒ˜ãƒƒãƒ€ãƒ¼æ›¸å¼ (ãƒ•ã‚©ãƒ³ãƒˆ: æ¸¸ã‚´ã‚·ãƒƒã‚¯, ç½«ç·šãªã—, å¤ªå­—)
                header_format = workbook.add_format({
                    'font_name': 'æ¸¸ã‚´ã‚·ãƒƒã‚¯',
                    'bold': True,
                    'border': 0  # ç½«ç·šãªã—
                })

                # è‰²ä»˜ãã‚»ãƒ«ã®æ›¸å¼ã‚’å‹•çš„ã«ä½œæˆ
                color_formats = {}
                for status, (bg_color, font_color) in EXCEL_COLOR_MAP.items():
                    color_formats[status] = workbook.add_format({
                        'font_name': 'æ¸¸ã‚´ã‚·ãƒƒã‚¯',
                        'bg_color': bg_color,
                        'font_color': font_color
                    })
                
                # --- 2. DataFrameã‚’Excelã«æ›¸ãè¾¼ã‚€ (ãƒ‡ãƒ¼ã‚¿ã®ã¿) ---
                # to_excelã§ãƒ‡ãƒ¼ã‚¿ã®ã¿æ›¸ãè¾¼ã‚€ (ãƒ˜ãƒƒãƒ€ãƒ¼ã¯å¾Œã§æ‰‹å‹•æç”»)
                df.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=1)
                
                # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
                worksheet = writer.sheets[sheet_name]

                # --- 3. ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ‰‹å‹•ã§æ›¸ãè¾¼ã‚€ (æ›¸å¼é©ç”¨ã®ãŸã‚) ---
                for col_num, value in enumerate(df.columns.values):
                    worksheet.write(0, col_num, value, header_format)

                # --- 4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ«ã«æ›¸å¼ã‚’é©ç”¨ ---
                # (to_excelã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ›¸å¼ã—ã‹é©ç”¨ã§ããªã„ãŸã‚ã€è‰²ä»˜ã‘ã®ãŸã‚ã«ä¸Šæ›¸ã)
                
                # PORTAL_ORDER ã¯ L.263 ä»˜è¿‘ã§å®šç¾©æ¸ˆã¿
                portal_cols = [p for p in PORTAL_ORDER if p in df.columns]
                check_col_name = 'ãƒã‚§ãƒƒã‚¯'
                
                # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆ
                for row_num in range(len(df)):
                    # ãƒ‡ãƒ¼ã‚¿è¡Œã®é–‹å§‹ã¯1è¡Œç›® (0è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼)
                    excel_row_idx = row_num + 1 
                    
                    # ã‚«ãƒ©ãƒ ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆ
                    for col_num, col_name in enumerate(df.columns):
                        value = df.iloc[row_num, col_num]
                        
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ›¸å¼ã‚’ã¾ãšé©ç”¨
                        cell_format = default_format
                        
                        # è‰²ä»˜ã‘å¯¾è±¡åˆ—ã‹åˆ¤å®š
                        if col_name in portal_cols and value in color_formats:
                            cell_format = color_formats.get(value, default_format)
                        elif col_name == check_col_name and value == 'è¦ç¢ºèª':
                            cell_format = color_formats.get('è¦ç¢ºèª', default_format)
                        
                        # ã‚»ãƒ«ã«å€¤ã¨æ›¸å¼ã‚’æ›¸ãè¾¼ã‚€
                        # (to_excelã§æ—¢ã«æ›¸ã‹ã‚ŒãŸå€¤ã‚’ä¸Šæ›¸ã)
                        worksheet.write(excel_row_idx, col_num, value, cell_format)

                # --- 5. åˆ—å¹…ã‚’è‡ªå‹•èª¿æ•´ ---
                # DataFrameã®åˆ—åã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã®è¾æ›¸ã‚’ä½œæˆ
                col_indices = {col_name: i for i, col_name in enumerate(df.columns)}

                # PORTAL_ORDER ã¯ L.263 ä»˜è¿‘ã§å®šç¾©æ¸ˆã¿
                portal_cols = [p for p in PORTAL_ORDER if p in col_indices]
                utility_cols = ['ãƒã‚§ãƒƒã‚¯', 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°', 'å…¬é–‹ä¸­ã®æ•°']

                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¹…
                default_width = 13 # (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ã‚„ã‚³ãƒ¼ãƒ‰ãªã©)

                # åˆ—ã”ã¨ã«å¹…ã‚’è¨­å®š
                for col_name, col_idx in col_indices.items():
                    width = default_width # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¹…ã‚’ã‚»ãƒƒãƒˆ
                    
                    if col_name == 'è¿”ç¤¼å“å':
                        # â˜… è¿”ç¤¼å“åã‚’ 60 ã«è¨­å®š (ç¾åœ¨ã®ç´„2/3ã‚’æƒ³å®š)
                        width = 60 
                    elif col_name == 'äº‹æ¥­è€…å':
                        # â˜… äº‹æ¥­è€…åã‚’ 25 ã«è¨­å®š (å°‘ã—åºƒã’ã‚‹)
                        width = 25 
                    elif col_name == 'äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰':
                        width = 15 # äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã¯å°‘ã—åºƒã‚
                    elif col_name not in portal_cols and col_name not in utility_cols:
                        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ä»¥å¤– (è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ãªã©)
                        width = 15 
                    
                    # set_column(first_col, last_col, width)
                    worksheet.set_column(col_idx, col_idx, width)
                
            # writer.close() ã¯ with ãƒ–ãƒ­ãƒƒã‚¯ãŒè‡ªå‹•ã§å‡¦ç†
            return output.getvalue()

        # --- CSVå¤‰æ›é–¢æ•° ---
        @st.cache_data
        def to_csv(df):
            # DataFrameã‚’ã¾ãšCSVæ–‡å­—åˆ—ã«å¤‰æ›
            # (to_csvã«encodingã‚’æŒ‡å®šã—ã¦ã‚‚æ–‡å­—åˆ—å‡ºåŠ›ã§ã¯ç„¡è¦–ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯æŒ‡å®šã—ãªã„)
            csv_string = df.to_csv(index=False) 
            
            # æ–‡å­—åˆ—ã‚’ cp932 ãƒã‚¤ãƒˆåˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            # â˜… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ããªã„æ–‡å­—ã¯ '?' ã«ç½®æ› (errors='replace')
            return csv_string.encode('cp932', errors='replace')

        with button_col:
            if not df_to_display.empty:
                
                # â˜… ã‚«ãƒ©ãƒ é–“ã®éš™é–“ã‚’ "small" (ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã¨åŒã˜) ã«è¨­å®š
                excel_col, csv_col = st.columns([1, 1], gap="small")

                # --- Excelä¿å­˜ãƒœã‚¿ãƒ³ã‚’1åˆ—ç›®ã«é…ç½® ---
                with excel_col:
                    excel_data = to_excel(df_to_display)
                    
                    # â˜… å¤‰æ›´: session_stateã‹ã‚‰å€¤ã‚’å–å¾—
                    # L708ã§ä¿å­˜ã—ãŸå€¤ã‚’ä½¿ç”¨ã€‚å­˜åœ¨ã—ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚‚è¨­å®š
                    base_portal_for_name = st.session_state.get('current_base_portal', 'N/A')
                    date_str_for_name = st.session_state.get('current_select_date_str', 'YYYYMMDD')
                    
                    # â˜… å¤‰æ›´: ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ–°ã—ã„å½¢å¼ã«
                    file_name_excel = f"æ²è¼‰çŠ¶æ³ãƒ‡ãƒ¼ã‚¿_{TODAY_STR}ï¼ˆtarget_{base_portal_for_name}_{date_str_for_name}ï¼‰.xlsx"
                    
                    st.download_button(
                        label="Excelä¿å­˜",
                        data=excel_data,
                        file_name=file_name_excel, # â˜… å¤‰æ›´
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key="excel_download",
                        width='stretch' 
                    )

                # --- CSVä¿å­˜ãƒœã‚¿ãƒ³ã‚’2åˆ—ç›®ã«é…ç½® ---
                with csv_col:
                    csv_data = to_csv(df_to_display)
                    
                    # â˜… å¤‰æ›´: session_stateã‹ã‚‰å€¤ã‚’å–å¾— (ä¸Šè¨˜ã¨åŒã˜å¤‰æ•°ã‚’ä½¿ç”¨)
                    base_portal_for_name = st.session_state.get('current_base_portal', 'N/A')
                    date_str_for_name = st.session_state.get('current_select_date_str', 'YYYYMMDD')
                    
                    # â˜… å¤‰æ›´: ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ–°ã—ã„å½¢å¼ã«
                    file_name_csv = f"æ²è¼‰çŠ¶æ³ãƒ‡ãƒ¼ã‚¿_{TODAY_STR}ï¼ˆtarget_{base_portal_for_name}_{date_str_for_name}ï¼‰.csv"
                    
                    st.download_button(
                        label="CSVä¿å­˜",
                        data=csv_data,
                        file_name=file_name_csv, # â˜… å¤‰æ›´
                        mime="text/csv",
                        key="csv_download",
                        width='stretch'
                    )

        # --- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°ã¨è¡¨ç¤º ---
        color_map = {'å…¬é–‹ä¸­': 'background-color: #22a579; color: white;', 'æœªç™»éŒ²': 'background-color: #111111; color: white;', 'å—ä»˜çµ‚äº†': 'background-color: #6c757d; color: white;', 
                     'éè¡¨ç¤º': 'background-color: #6c757d; color: white;', 'åœ¨åº«0': 'background-color: #6c757d; color: white;', 'å€‰åº«': 'background-color: #6c757d; color: white;',
                     'æœªå—ä»˜': 'background-color: #ffc107; color: black;'}
        
        def style_dataframe(df):
            style = pd.DataFrame('', index=df.index, columns=df.columns)
            portal_cols = [p for p in PORTAL_ORDER if p in df.columns]
            for col in portal_cols: style[col] = df[col].map(color_map).fillna('')
            if 'ãƒã‚§ãƒƒã‚¯' in df.columns: style['ãƒã‚§ãƒƒã‚¯'] = df['ãƒã‚§ãƒƒã‚¯'].apply(lambda x: 'background-color: #fa6c78; color: black;' if x == 'è¦ç¢ºèª' else '')
            return style

        # --- ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒ©ã‚¤ã‚¹ã¨æç”» ---
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒ0ä»¶ã§ãªã„å ´åˆã®ã¿ã‚¹ãƒ©ã‚¤ã‚¹ã¨æç”»ã‚’å®Ÿè¡Œ
        # (0ä»¶ã®å ´åˆã¯ L.1138 ã® st.warning ãŒè¡¨ç¤ºã•ã‚Œã‚‹æƒ³å®š)
        if not df_to_display.empty:
        
            # ãƒšãƒ¼ã‚¸ç•ªå·ã‹ã‚‰ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
            # (current_page ã¯ L.1193 ã§è£œæ­£æ¸ˆã¿)
            start_idx = (current_page - 1) * ITEMS_PER_PAGE
            end_idx = start_idx + ITEMS_PER_PAGE
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹
            df_sliced = df_to_display.iloc[start_idx:end_idx]
            
            # â˜… ã‚¹ãƒ©ã‚¤ã‚¹ã—ãŸDFã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¨ä½“ã®é€£ç•ªã«å¤‰æ›´
            df_sliced.index = range(start_idx + 1, start_idx + 1 + len(df_sliced))
            
            # ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°å¯¾è±¡ã®ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ
            center_aligned_cols = [p for p in PORTAL_ORDER if p in df_sliced.columns] + ['ãƒã‚§ãƒƒã‚¯', 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°', 'å…¬é–‹ä¸­ã®æ•°']

            # â˜… ã‚¹ãƒ©ã‚¤ã‚¹ã—ãŸ df_sliced ã«å¯¾ã—ã¦ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
            styler = df_sliced.style.apply(style_dataframe, axis=None).set_properties(subset=center_aligned_cols, **{'text-align': 'center'})

            # â˜… ã‚¹ãƒ©ã‚¤ã‚¹ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æç”»
            st.dataframe(styler, width='stretch', height=800)

        # --- ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³UI (è¡¨ã®ä¸‹ã«é…ç½®) ---
        # total_items, total_pages, current_page ã¯ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç›´å¾Œ(L.1184ä»˜è¿‘)ã§è¨ˆç®—æ¸ˆã¿
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒ0ä»¶ã§ãªã„å ´åˆã®ã¿ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
        if total_items > 0:
            # (ITEMS_PER_PAGE, total_pages, current_page ã¯ L.1184 ä»˜è¿‘ã§å®šç¾©ãƒ»è¨ˆç®—æ¸ˆã¿)
            
            # [å…¨ä»¶æ•°è¡¨ç¤º] [ [n] / n ãƒšãƒ¼ã‚¸] 
            col_spacer, col_page_input, col_page_total, col_spacer_end, col_max_num = st.columns([
                3.5,  # ç©ºç™½ (èª¿æ•´)
                0.8, # [n] (å…¥åŠ›æ¬„)
                0.9, # / n ãƒšãƒ¼ã‚¸ (ãƒ†ã‚­ã‚¹ãƒˆ)
                1.5,  # ç©ºç™½ (èª¿æ•´)
                1.5  # æœ€å¤§ä»¶æ•°èª¬æ˜æ–‡
            ])

            # ãƒšãƒ¼ã‚¸ç•ªå·å…¥åŠ›ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
            # (st.number_input ã® on_change ã§å‘¼ã³å‡ºã•ã‚Œã‚‹)
            def update_page_number():
                # number_inputã®å€¤(page_input_box)ã‚’current_pageã«åæ˜ ã™ã‚‹
                if st.session_state.page_input_box != st.session_state.current_page:
                    st.session_state.current_page = st.session_state.page_input_box
                    # on_change ãŒç™ºç«ã™ã‚‹ã¨ Streamlit ãŒè‡ªå‹•ã§ rerun ã™ã‚‹ãŸã‚ã€st.rerun() ã¯ä¸è¦

            with col_page_input:
                # ãƒšãƒ¼ã‚¸ç•ªå·å…¥åŠ›æ¬„
                st.number_input(
                    label="ãƒšãƒ¼ã‚¸ç•ªå·",
                    min_value=1,
                    max_value=total_pages,
                    value=current_page, # â˜… è¡¨ç¤ºã™ã‚‹å€¤ã¯å¸¸ã« current_page
                    step=1,
                    key="page_input_box", # â˜… key ã‚’ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å‚ç…§ç”¨ã®åˆ¥åã«å¤‰æ›´
                    on_change=update_page_number, # å¤‰æ›´æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
                    label_visibility="collapsed",
                    help=f"1ï½{total_pages} ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å…¥åŠ›"
                )

            with col_page_total:
                # ç·ãƒšãƒ¼ã‚¸æ•°è¡¨ç¤º (å·¦æƒãˆã«ã—ã¦å…¥åŠ›æ¬„ã®ã™ãå³ã«é…ç½®)
                st.markdown(
                    f"<div style='margin-top: 8px; text-align: left; font-weight: bold;'> / {total_pages} ãƒšãƒ¼ã‚¸</div>",
                    unsafe_allow_html=True
                )
            
            with col_max_num:
                # ãƒšãƒ¼ã‚¸ç•ªå·è¡¨ç¤º (ä¸­å¤®æƒãˆ)
                st.markdown(
                    f"<div style='margin-top: 8px; text-align: center; font-weight: 500;'>â€»1ãƒšãƒ¼ã‚¸æœ€å¤§500ä»¶è¡¨ç¤º</div>",
                    unsafe_allow_html=True
                )

        st.markdown("---")
        
        # ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ã®ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°
        if 'confirming_reset' not in st.session_state:
            st.session_state.confirming_reset = False

        if st.session_state.confirming_reset:
            st.warning("æœ¬å½“ã«æ²è¼‰çŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ")
            
            col1, col2, _ = st.columns([1, 1, 8], gap="small") 
            
            with col1:
                if st.button("OK", key="reset_confirm_ok", width='stretch'):
                    # å®Ÿè¡Œå‡¦ç†
                    if 'dataframes' in st.session_state:
                        meta_keys = [k for k in st.session_state.dataframes if k.endswith('_metadata')]
                        for k in meta_keys:
                            del st.session_state.dataframes[k] # è¾æ›¸ã®ä¸­èº«ã‚’å‰Šé™¤

                    keys_to_clear = [
                        'results_df', 'dataframes', 'choice_stock_processed', 'rakuten_merged',
                        'current_select_date_str', 'current_base_portal'
                    ]
                    for key in keys_to_clear:
                        if key in st.session_state:
                            del st.session_state[key] # å±æ€§è‡ªä½“ã‚’å‰Šé™¤

                    st.session_state.uploader_key += 1
                    
                    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã®ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                    st.session_state.show_reset_success = True
                    
                    # çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†æç”»
                    st.session_state.confirming_reset = False
                    st.rerun()
            with col2:
                if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="reset_confirm_cancel", width='stretch'):
                    st.session_state.confirming_reset = False
                    st.rerun()
        else:
            if st.button("æ²è¼‰çŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆ"):
                st.session_state.confirming_reset = True
                st.rerun()