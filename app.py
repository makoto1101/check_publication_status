import streamlit as st
import pandas as pd
import numpy as np
import re
from io import BytesIO
from datetime import datetime
import os
import json

from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# --- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from status import calculate_status
# --- æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from operation_manual import show_instructions
# --- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šæ¡ä»¶ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from status_manual import show_status_conditions

# åŸºæº–æ—¥ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«åç”¨ã®æ—¥ä»˜ã‚’å®šç¾©
TODAY = datetime.now().date()
TODAY_STR = TODAY.strftime('%Y%m%d')

def local_css(file_name):
    """å¤–éƒ¨CSSãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®é–¢æ•°"""
    try:
        with open(file_name, "r", encoding="utf-8") as f:
            st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
    except FileNotFoundError:
        st.error(f"CSSãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")

# --- ãƒšãƒ¼ã‚¸è¨­å®š (ãƒ¯ã‚¤ãƒ‰ãƒ¢ãƒ¼ãƒ‰) ---
st.set_page_config(layout="wide", page_title="æ²è¼‰çŠ¶æ³ç¢ºèªã‚¢ãƒ—ãƒª", page_icon="ğŸ“Š")

# --- ãƒšãƒ¼ã‚¸ä¸Šéƒ¨ã®ä½™ç™½ã‚’ãªãã™CSSã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ­ã‚°ã‚¤ãƒ³çŠ¶æ…‹ã«é–¢ã‚ã‚‰ãšè¡¨ç¤º ---
st.markdown("""
    <style>
            .block-container {
                padding-top: 1rem;
            }
            /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®å¹…ã‚’600pxã«æŒ‡å®š */
            [data-testid="stSidebar"] {
                width: 600px !important;
            }
    </style>
""", unsafe_allow_html=True)

# CSSã®é©ç”¨
local_css("style.css")

# UIãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
st.title("ğŸ“Š æ²è¼‰çŠ¶æ³ç¢ºèªã‚¢ãƒ—ãƒª")
# --------------------------------------------------------------------

# --- Streamlit çµ„ã¿è¾¼ã¿èªè¨¼ ---

# 1. æœªãƒ­ã‚°ã‚¤ãƒ³ã®å ´åˆ: ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
if not st.user.get("is_logged_in", False):
    
    # ã‚«ãƒ©ãƒ ã‚’ä½¿ã£ã¦ä¸­å¤®ã«é…ç½®
    _, form_col, _ = st.columns([1.5, 1, 1.5])
    with form_col:
        # ä¸Šéƒ¨ã«30pxã®ä½™ç™½ã‚’è¿½åŠ 
        st.markdown("<div style='margin-top: 80px;'></div>", unsafe_allow_html=True)
        
        st.warning('Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚')
        
        # 1. ã¾ãšStreamlitã®ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹
        if st.button("Googleã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ãƒ­ã‚°ã‚¤ãƒ³", icon=":material/login:", width='stretch'):
            # 2. ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰ã€st.login() ã‚’å‘¼ã³å‡ºã™ï¼ˆèªè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ï¼‰
            st.login() 
    
    # æœªãƒ­ã‚°ã‚¤ãƒ³æ™‚ã¯ã“ã“ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œã‚’åœæ­¢ã™ã‚‹
    st.stop()

# 2. ãƒ­ã‚°ã‚¤ãƒ³æ¸ˆã¿ã®å ´åˆ: ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚’è¡¨ç¤º
else:
    # --- ãƒ˜ãƒƒãƒ€ãƒ¼ã‚¨ãƒªã‚¢ï¼ˆãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ¡ä»¶ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼åãƒ»ãƒ­ã‚°ã‚¢ã‚¦ãƒˆï¼‰ ---
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆèª¿æ•´: [æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«] [åˆ¤å®šæ¡ä»¶] [ç©ºç™½] [ãƒ¦ãƒ¼ã‚¶ãƒ¼å] [ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ]
    # ã‚«ãƒ©ãƒ æ¯”ç‡ã‚’èª¿æ•´ã—ã¦ãƒœã‚¿ãƒ³ã‚’ä¸¦ã¹ã‚‹
    col_manual, col_status_link, _, col_user, col_logout = st.columns([2, 2.5, 3, 4.5, 2.2], gap="small")
    
    # 1. æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«ï¼ˆãƒªãƒ³ã‚¯é¢¨ãƒœã‚¿ãƒ³ï¼‰
    with col_manual:
        st.markdown("<div style='margin-top: 15px;'>", unsafe_allow_html=True)
        if st.button("ğŸ“– æ“ä½œãƒãƒ‹ãƒ¥ã‚¢ãƒ«", type="tertiary"):
            show_instructions()
        st.markdown("</div>", unsafe_allow_html=True)

    # 2. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šæ¡ä»¶ï¼ˆãƒªãƒ³ã‚¯é¢¨ãƒœã‚¿ãƒ³ï¼‰(â˜…è¿½åŠ )
    with col_status_link:
        st.markdown("<div style='margin-top: 15px;'>", unsafe_allow_html=True)
        if st.button("ğŸ“‹ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šæ¡ä»¶", type="tertiary"):
            show_status_conditions()
        st.markdown("</div>", unsafe_allow_html=True)

    # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼åè¡¨ç¤º
    with col_user:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¯ st.user.name ã§å–å¾—å¯èƒ½
        st.markdown(f"<div style='text-align: right; margin-top: 22px;'>ã‚ˆã†ã“ã <b>{st.user.name}</b> ã•ã‚“</div>", unsafe_allow_html=True)
    
    # 4. ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
    with col_logout:
        st.markdown("<div style='margin-top: 0px;'>", unsafe_allow_html=True)
        if st.button("ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ", width='stretch'):
            st.logout()
        st.markdown("</div>", unsafe_allow_html=True)
    # ----------------------------------------------------

    # UIï¼ˆst.sidebarï¼‰ã‚ˆã‚Šå…ˆã«ã€å¿…è¦ãªé–¢æ•°ã‚„å¤‰æ•°ã‚’å®šç¾©ã™ã‚‹

    # --- Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š ---
    GSHEET_KEY = "1Yb-0DLDb-IAKIxDkhaSZxDl-zd2iDHZ3aX3_4mSiQyI"

    # --- Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æºé–¢æ•° ---
    @st.cache_resource(ttl=600) # 10åˆ†é–“ service ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    def init_sheets_service():
        """Google Sheets API ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹"""
        try:
            # st.secretsã‹ã‚‰èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
            google_credentials_info = json.loads(st.secrets["gcp_service_account"]["credentials_json"])
            
            # App 2 ã¨åŒã˜ 'readonly' ã‚¹ã‚³ãƒ¼ãƒ—ã‚’ä½¿ç”¨
            scopes = [
                'https://www.googleapis.com/auth/spreadsheets.readonly'
                # 'drive' ã‚¹ã‚³ãƒ¼ãƒ—ã¯ã‚·ãƒ¼ãƒˆã®èª­ã¿å–ã‚Šã ã‘ãªã‚‰ä¸è¦
            ]
            
            credentials = service_account.Credentials.from_service_account_info(
                google_credentials_info,
                scopes=scopes
            )
            
            service = build('sheets', 'v4', credentials=credentials)
            return service

        except KeyError:
            st.error("Googleèªè¨¼æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`.streamlit/secrets.toml` ã« `[google]` ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ `credentials_json` ã‚­ãƒ¼ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None
        except json.JSONDecodeError:
            st.error("Googleèªè¨¼æƒ…å ±ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚`.streamlit/secrets.toml` ã® `credentials_json` ãŒæœ‰åŠ¹ãªJSONæ–‡å­—åˆ—ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None
        except Exception as e:
            st.error(f"[init_sheets_service] Google Sheets ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            st.error("`.streamlit/secrets.toml` ã®è¨­å®šãŒæ­£ã—ã„ã‹ã€GCPã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒæœ‰åŠ¹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return None
    
    def get_data_from_gsheet(_sheets_service, sheet_name, expected_headers):
        """æŒ‡å®šã•ã‚ŒãŸã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿DataFrameã¨ã—ã¦è¿”ã™ (googleapiclientç‰ˆ)"""
        try:
            # ã‚·ãƒ¼ãƒˆå…¨ä½“ (A1ã‹ã‚‰æœ€å¾Œã¾ã§) ã‚’å–å¾—ã™ã‚‹
            range_name = f"{sheet_name}!A1:ZZ" # å¿µã®ãŸã‚ZZåˆ—ã¾ã§
            
            result = _sheets_service.spreadsheets().values().get(
                spreadsheetId=GSHEET_KEY, 
                range=range_name
            ).execute()
            
            data = result.get('values', [])
            # â˜… gspread ã¨åŒã˜ 
            
            if not data:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚·ãƒ¼ãƒˆãŒç©ºã®ã‚ˆã†ã§ã™ï¼‰ã€‚")
                return pd.DataFrame(columns=expected_headers)

            headers = data[0]
            # å¿…è¦ãªåˆ—ã ã‘ã‚’æŠ½å‡º
            cols_to_use = []
            col_indices = []

            for header in expected_headers:
                try:
                    idx = headers.index(header)
                    cols_to_use.append(header)
                    col_indices.append(idx)
                except ValueError:
                    st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã«å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ '{header}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    return pd.DataFrame(columns=expected_headers)

            # 2è¡Œç›®ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            records = [
                tuple(row[i] for i in col_indices if i < len(row)) # â˜… è¡Œæœ«è¶…ãˆã‚¨ãƒ©ãƒ¼ã‚’é˜²æ­¢
                for row in data[1:]
            ]
            
            df = pd.DataFrame(records, columns=cols_to_use)
            return df

        except HttpError as err:
            # HttpError ã‚’ã‚­ãƒ£ãƒƒãƒ
            if err.resp.status == 404:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            elif err.resp.status == 403:
                st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒå…±æœ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            else:
                st.error(f"[get_data_from_gsheet] ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®èª­ã¿è¾¼ã¿ä¸­ã« HttpError ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {err}")
            return pd.DataFrame(columns=expected_headers)
        except Exception as e:
            st.error(f"[get_data_from_gsheet] ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ '{sheet_name}' ã®èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return pd.DataFrame(columns=expected_headers)

    # --- å„DBç”¨ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•° (Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆç‰ˆ) ---
    @st.cache_data(ttl=600) # 10åˆ†é–“ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    def get_teiki_data_from_gsheet(_sheets_service):
        """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰å®šæœŸä¾¿ç•ªå·ã®ã‚»ãƒƒãƒˆã‚’å–å¾—ã™ã‚‹"""
        if _sheets_service is None: return None
        
        sheet_name = "å®šæœŸä¾¿DB"
        expected_headers = ["å®šæœŸä¾¿ç•ªå·"]
        df = get_data_from_gsheet(_sheets_service, sheet_name, expected_headers)
        
        if df.empty:
            return set() # ç©ºã®ã‚»ãƒƒãƒˆã‚’è¿”ã™
            
        return set(df["å®šæœŸä¾¿ç•ªå·"].dropna().unique())

    @st.cache_data(ttl=600)
    def get_business_data_from_gsheet(_sheets_service):
        """Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰äº‹æ¥­è€…ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã¨ã—ã¦å–å¾—ã™ã‚‹"""
        if _sheets_service is None: return pd.DataFrame(columns=["äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰", "äº‹æ¥­è€…å", "è‡ªæ²»ä½“å"])
        
        sheet_name = "äº‹æ¥­è€…DB"
        expected_headers = ["äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰", "äº‹æ¥­è€…å", "è‡ªæ²»ä½“å"]
        df = get_data_from_gsheet(_sheets_service, sheet_name, expected_headers)
        return df

    # gspreadã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ– -> sheetsã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–
    sheets_service = init_sheets_service()

    # è¿”ç¤¼å“ã€Œã‚³ãƒ¼ãƒ‰ã€åˆ—ã®å®šç¾©ï¼ˆãƒãƒ§ã‚¤ã‚¹ç³»ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã€ä»–ã¯ãƒ˜ãƒƒãƒ€ãƒ¼åï¼‰
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ãƒ˜ãƒƒãƒ€ãƒ¼åãƒªã‚¹ãƒˆã«åŸºã¥ãå®šç¾©
    KEY_COLUMN_MAP = {
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·)
        "ãƒãƒ§ã‚¤ã‚¹": 102,
        "ãƒãƒ§ã‚¤ã‚¹åœ¨åº«": 0,
        
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Š: ãƒ˜ãƒƒãƒ€ãƒ¼å)
        "æ¥½å¤©": "å•†å“ç•ªå·",
        "ANA": "è¿”ç¤¼å“è­˜åˆ¥ã‚³ãƒ¼ãƒ‰",
        "ãµã‚‹ãªã³": "å¤–éƒ¨è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 19)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "JAL": "è¿”ç¤¼å“ç•ªå·",
        "ã¾ã„ãµã‚‹": "è¿”ç¤¼å“ç•ªå·",
        "ãƒã‚¤ãƒŠãƒ“": "è¿”ç¤¼å“ç•ªå·",
        "ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ": "SKU", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 5)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "JRE": "å“ç•ª1", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 2)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "ã•ã¨ãµã‚‹": "ãŠç¤¼å“å", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 1, [code]æŠ½å‡º)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "ã•ã¨ãµã‚‹åœ¨åº«": "ãŠç¤¼å“ID", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 1)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "Amazon": "å‡ºå“è€…SKU", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 0)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "ç™¾é¸": "è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰",
        "ç™¾é¸åœ¨åº«": "è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰",
        "ãã‚‹ãªã³": "å•†å“ç•ªå·"
    }

    # è¿”ç¤¼å“ã€Œåç§°ã€åˆ—ã®å®šç¾©ï¼ˆãƒãƒ§ã‚¤ã‚¹ç³»ã¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã€ä»–ã¯ãƒ˜ãƒƒãƒ€ãƒ¼åï¼‰
    PORTAL_NAME_COLUMN_MAP = {
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·)
        "ãƒãƒ§ã‚¤ã‚¹": 2,
        
        # (ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Š: ãƒ˜ãƒƒãƒ€ãƒ¼å)
        "æ¥½å¤©": "å•†å“å",
        "ANA": "è¿”ç¤¼å“å",
        "ãµã‚‹ãªã³": "è¿”ç¤¼å“å",
        "JAL": "è¿”ç¤¼å“å",
        "ã¾ã„ãµã‚‹": "è¿”ç¤¼å“å",
        "ãƒã‚¤ãƒŠãƒ“": "è¿”ç¤¼å“å",
        "ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ": "è¿”ç¤¼å“å",
        "JRE": "å•†å“å",
        "ã•ã¨ãµã‚‹": "ãŠç¤¼å“å", # æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯(index 1)ã¨ãƒ˜ãƒƒãƒ€ãƒ¼ãƒªã‚¹ãƒˆã‚’ç…§åˆ
        "Amazon": None,
        "ç™¾é¸": "è¿”ç¤¼å“åç§°",
        "ãã‚‹ãªã³": "å•†å“å"
    }

    PORTAL_ORDER = ['ãƒãƒ§ã‚¤ã‚¹', 'æ¥½å¤©', 'ANA', 'ãµã‚‹ãªã³', 'JAL', 'ã¾ã„ãµã‚‹', 'ãƒã‚¤ãƒŠãƒ“', 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ', 'JRE', 'ã•ã¨ãµã‚‹', 'Amazon', 'ç™¾é¸', 'ãã‚‹ãªã³']
    # TODAY_STR ã¯ L23 ã§å®šç¾©

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‚·ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
    SKIP_FILTERING_SHEETS = ['æ¥½å¤©', 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«', 'ã•ã¨ãµã‚‹åœ¨åº«', 'ç™¾é¸åœ¨åº«'] # ã€Œæ¥½å¤©ã€ãŒãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆãŒç‰¹æ®Šãªã®ã§ã‚¹ã‚­ãƒƒãƒ—
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æŒãŸãªã„ï¼ˆ`header=None`ã§èª­ã¿è¾¼ã‚€ï¼‰ã‚·ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
    SHEETS_WITHOUT_HEADER = ['ãƒãƒ§ã‚¤ã‚¹', 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«']


    # --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
    def get_sheet_name_from_filename(filename):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚·ãƒ¼ãƒˆåã‚’æ¨æ¸¬ã™ã‚‹"""
        name_lower = filename.lower()
        if 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«' in name_lower: return 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«'
        if 'ãƒãƒ§ã‚¤ã‚¹' in name_lower: return 'ãƒãƒ§ã‚¤ã‚¹'
        if 'æ¥½å¤©' in name_lower: return 'æ¥½å¤©'
        if 'ana' in name_lower: return 'ANA'
        if 'ãµã‚‹ãªã³' in name_lower: return 'ãµã‚‹ãªã³'
        if 'jal' in name_lower: return 'JAL'
        if 'ã¾ã„ãµã‚‹' in name_lower: return 'ã¾ã„ãµã‚‹'
        if 'ãƒã‚¤ãƒŠãƒ“' in name_lower: return 'ãƒã‚¤ãƒŠãƒ“'
        if 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ' in name_lower: return 'ãƒ—ãƒ¬ãƒŸã‚¢ãƒ '
        if 'jre' in name_lower: return 'JRE'
        if 'ã•ã¨ãµã‚‹åœ¨åº«' in name_lower: return 'ã•ã¨ãµã‚‹åœ¨åº«'
        if 'ã•ã¨ãµã‚‹' in name_lower: return 'ã•ã¨ãµã‚‹'
        if 'amazon' in name_lower: return 'Amazon'
        if 'ç™¾é¸åœ¨åº«' in name_lower: return 'ç™¾é¸åœ¨åº«'
        if 'ç™¾é¸' in name_lower: return 'ç™¾é¸'
        if 'ãã‚‹ãªã³' in name_lower: return 'ãã‚‹ãªã³'
        
        # ãƒãƒ¼ã‚¿ãƒ«åã‚’ç‰¹å®šã§ããªã„å ´åˆã¯ None ã‚’è¿”ã™ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡å¤–ï¼‰
        return None

    def robust_read_file(uploaded_file):
        """
        æ§˜ã€…ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨å½¢å¼ã«å¯¾å¿œã—ãŸãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°ã€‚
        ã‚·ãƒ¼ãƒˆåã«å¿œã˜ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã®æœ‰ç„¡ï¼ˆheader=0 or header=Noneï¼‰ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚
        """
        bytes_data = uploaded_file.getvalue()
        file_name = uploaded_file.name
        sheet_name = get_sheet_name_from_filename(file_name)
        
        # ã‚·ãƒ¼ãƒˆåã«åŸºã¥ã„ã¦ãƒ˜ãƒƒãƒ€ãƒ¼ã®æœ‰ç„¡ã‚’æ±ºå®š
        # ãƒãƒ§ã‚¤ã‚¹ç³»ã¯ header=Noneã€ãã‚Œä»¥å¤–ã¯ header=0 (1è¡Œç›®ã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã™ã‚‹)
        header_setting = None if sheet_name in SHEETS_WITHOUT_HEADER else 0

        if file_name.endswith('.xlsx'):
            try:
                return pd.read_excel(BytesIO(bytes_data), header=header_setting, dtype=str).fillna('')
            except Exception as e:
                st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                return None

        separator = '\t' if file_name.lower().endswith(('.tsv', '.txt')) else ','
        
        if 'æ¥½å¤©' in file_name.lower() or 'ã•ã¨ãµã‚‹' in file_name.lower():
            encodings_to_try = ['shift_jis', 'utf-8']
        elif any(n.lower() in file_name.lower() for n in ["N2", "ãƒãƒ§ã‚¤ã‚¹", "ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ", "amazon"]):
            encodings_to_try = ['utf-8', 'shift_jis']
        else:
            encodings_to_try = ['shift_jis', 'utf-8']

        for encoding in encodings_to_try:
            try:
                corrected_encoding = 'utf-8-sig' if encoding == 'utf-8' else encoding
                df = pd.read_csv(
                    BytesIO(bytes_data), 
                    header=header_setting,
                    encoding=corrected_encoding, 
                    dtype=str, 
                    sep=separator, 
                    # engine='python',  <-- â˜…å‰Šé™¤ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚Cã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ï¼‰
                    on_bad_lines='warn', 
                    encoding_errors='ignore'
                )
                return df.fillna('')
            except Exception:
                bytes_data = uploaded_file.getvalue()
                continue
                
        st.error(f"'{file_name}' ã‚’ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return None

    def generate_vendor_code(item_code):
        """è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‹ã‚‰äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹"""
        code = str(item_code).strip()
        if not code: return ''
        if re.match(r'^\d{2}[A-Z]{4}', code): return code[:6]
        if re.match(r'^[A-Z]{4}', code): return code[:4]
        if re.match(r'^[A-Z]{3}', code): return code[:3]
        return ''

    def filter_dataframe(df, sheet_name, item_codes_to_filter, vendor_codes_to_filter):
        """
        DataFrameã‚’æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹é–¢æ•°ã€‚
        æ¥½å¤©ã®å ´åˆã¯ã€Œå•†å“ç•ªå·ã€ã¾ãŸã¯ã€Œã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·ã€ã‚’å¯¾è±¡ã¨ã™ã‚‹ã€‚
        è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã€äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰å…±ã«ã€Œéƒ¨åˆ†ä¸€è‡´ï¼ˆå«ã‚€ï¼‰ã€ã§åˆ¤å®šã™ã‚‹ã€‚
        """
        if df is None or df.empty:
            return df
        if not item_codes_to_filter and not vendor_codes_to_filter:
            return df
        
        data = df # robust_read_file ã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿ã®ãŸã‚ã€dfå…¨ä½“ãŒãƒ‡ãƒ¼ã‚¿
        if data.empty:
            return df

        # â˜… è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰æ¤œç´¢ç”¨ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ (éƒ¨åˆ†ä¸€è‡´ç”¨)
        item_pattern = ""
        if item_codes_to_filter:
            item_pattern = '|'.join(map(re.escape, item_codes_to_filter))

        # â˜… äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰æ¤œç´¢ç”¨ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ (éƒ¨åˆ†ä¸€è‡´ç”¨)
        vendor_pattern = ""
        if vendor_codes_to_filter:
            vendor_pattern = '|'.join(map(re.escape, vendor_codes_to_filter))

        # --- æ¥½å¤©ã®å ´åˆã®ç‰¹ä¾‹å‡¦ç† ---
        if sheet_name == 'æ¥½å¤©':
            # å¿…è¦ãªåˆ—ã®å­˜åœ¨ç¢ºèª (robust_read_fileå¾Œã®ãŸã‚é€šå¸¸ã¯ã‚ã‚‹ã¯ãšã ãŒå®‰å…¨ã®ãŸã‚get)
            col_item_no = "å•†å“ç•ªå·"
            col_sys_sku = "ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·"
            
            # åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã›ãšã«è¿”ã™ï¼ˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼æ‰±ã„ã§ã‚‚ã‚ˆã„ãŒã€ã“ã“ã§ã¯å®‰å…¨ç­–ï¼‰
            if col_item_no not in data.columns:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{sheet_name}' ã« '{col_item_no}' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return df
            
            # æ¯”è¼ƒç”¨ã«ã‚·ãƒªãƒ¼ã‚ºã‚’å–å¾— (ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·ãŒãªã„å ´åˆã‚‚è€ƒæ…®ã—ã¦get)
            series_item_no = data[col_item_no].astype(str).str.strip()
            if col_sys_sku in data.columns:
                series_sys_sku = data[col_sys_sku].astype(str).str.strip()
            else:
                # åˆ—ãŒãªã„å ´åˆã¯ãƒãƒƒãƒã—ãªã„ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ç©ºæ–‡å­—ã‚·ãƒªãƒ¼ã‚ºã‚’ä½œæˆ
                series_sys_sku = pd.Series('', index=data.index)

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ãƒã‚¹ã‚¯ã‚’åˆæœŸåŒ– (ã™ã¹ã¦True)
            mask = pd.Series(True, index=data.index)

            # 1. è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (éƒ¨åˆ†ä¸€è‡´: å•†å“ç•ªå· OR ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·)
            if item_pattern:
                # å•†å“ç•ªå·ãŒå«ã¾ã‚Œã‚‹ OR ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·ãŒå«ã¾ã‚Œã‚‹ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
                match_item = series_item_no.str.contains(item_pattern, na=False, case=False)
                match_sku = series_sys_sku.str.contains(item_pattern, na=False, case=False)
                mask &= (match_item | match_sku)

            # 2. äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (éƒ¨åˆ†ä¸€è‡´: å•†å“ç•ªå·ç”±æ¥ OR ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·ç”±æ¥)
            if vendor_pattern:
                # ãã‚Œãã‚Œã‹ã‚‰äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
                vendor_series_item = series_item_no.apply(generate_vendor_code)
                vendor_series_sku = series_sys_sku.apply(generate_vendor_code)
                
                # æ­£è¦è¡¨ç¾ã§éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
                match_vendor_item = vendor_series_item.str.contains(vendor_pattern, na=False, case=False)
                match_vendor_sku = vendor_series_sku.str.contains(vendor_pattern, na=False, case=False)
                
                mask &= (match_vendor_item | match_vendor_sku)

            return data[mask]

        # --- ä»¥ä¸‹ã€æ—¢å­˜ã®ä»–ãƒãƒ¼ã‚¿ãƒ«ç”¨ãƒ­ã‚¸ãƒƒã‚¯ ---
        
        key_col = KEY_COLUMN_MAP.get(sheet_name)
        if key_col is None:
            # ã‚­ãƒ¼åˆ—ãŒæœªå®šç¾©ãªã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãªã„
            return df
        
        item_code_series = pd.Series(dtype=str)
        
        # --- ã‚­ãƒ¼åˆ—ã®å‹ï¼ˆint or strï¼‰ã§å‡¦ç†ã‚’åˆ†å² ---
        if isinstance(key_col, int):
            # (ãƒãƒ§ã‚¤ã‚¹ç³»: ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã§å‚ç…§)
            if df.shape[1] <= key_col:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{sheet_name}' ã®åˆ—æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚­ãƒ¼åˆ— {key_col} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                return df
            
            # ãƒãƒ§ã‚¤ã‚¹ç³»ã¯ãã®ã¾ã¾ã®å€¤ã‚’ä½¿ç”¨
            item_code_series = data.iloc[:, key_col].astype(str).str.strip()

        elif isinstance(key_col, str):
            # (ãã®ä»–: ãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šã€ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
            if key_col not in data.columns:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{sheet_name}' ã«å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ '{key_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                return df
            
            # ã•ã¨ãµã‚‹ã®å ´åˆã€æ­£è¦è¡¨ç¾ã§ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            if sheet_name == 'ã•ã¨ãµã‚‹':
                # 'ãŠç¤¼å“å' åˆ—(key_col)ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º [xxxx] -> xxxx
                item_code_series = data[key_col].astype(str).str.extract(r'\[(.*?)\]', expand=False).fillna('')
            else:
                # ä»–ãƒãƒ¼ã‚¿ãƒ«ã¯ãã®ã¾ã¾ã®å€¤ã‚’ä½¿ç”¨
                item_code_series = data[key_col].astype(str).str.strip()
        
        else:
            # key_col ãŒ None ã¾ãŸã¯äºˆæœŸã›ã¬å‹
            return df

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®ãƒã‚¹ã‚¯ã‚’åˆæœŸåŒ–
        mask = pd.Series(True, index=data.index)
        
        # 1. è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (éƒ¨åˆ†ä¸€è‡´)
        if item_pattern:
            # æ­£è¦è¡¨ç¾ã§éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
            mask &= (item_code_series != '') & (item_code_series.str.contains(item_pattern, na=False, case=False))
        
        # 2. äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (éƒ¨åˆ†ä¸€è‡´)
        if vendor_pattern:
            # æŠ½å‡ºã¾ãŸã¯å–å¾—ã—ãŸè¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚·ãƒªãƒ¼ã‚ºã‹ã‚‰äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
            vendor_code_series = item_code_series.apply(generate_vendor_code)
            # æ­£è¦è¡¨ç¾ã§éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ (å¤§æ–‡å­—å°æ–‡å­—ç„¡è¦–)
            mask &= (vendor_code_series != '') & (vendor_code_series.str.contains(vendor_pattern, na=False, case=False))
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã¯ robust_read_file ã§å‡¦ç†æ¸ˆã¿ (df.columns ã«æ ¼ç´)
        # ãã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿è¡Œ (data) ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦è¿”ã™
        return data[mask]

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– (ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªç”¨)
    if 'uploader_key' not in st.session_state:
        st.session_state.uploader_key = 0
    if 'dataframes' not in st.session_state:
        st.session_state.dataframes = {}
    if 'results_df' not in st.session_state:
        st.session_state.results_df = pd.DataFrame()
    # (èªè¨¼é–¢é€£ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã¯StreamlitãŒå†…éƒ¨ã§ç®¡ç†ã™ã‚‹ãŸã‚ä¸è¦)

    # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çŠ¶æ…‹ã®åˆæœŸåŒ– (â˜… è¿½åŠ : ãƒªã‚»ãƒƒãƒˆã•ã‚Œãªã„ã‚ˆã†ã«session_stateã§ç®¡ç†) ---
    if 'f_search' not in st.session_state: st.session_state.f_search = ""
    if 'f_vendor' not in st.session_state: st.session_state.f_vendor = "ã™ã¹ã¦"
    if 'f_check' not in st.session_state: st.session_state.f_check = "ã™ã¹ã¦"
    if 'f_teiki' not in st.session_state: st.session_state.f_teiki = "ã™ã¹ã¦"

    # --- ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆ ---
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 1
    # ----------------------------------------

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼UIã‚»ã‚¯ã‚·ãƒ§ãƒ³
    with st.sidebar:
        st.markdown('<h2 style="font-size: 24px;">1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†</h2>', unsafe_allow_html=True)

        with st.expander("DBï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰ã‚’é–‹ã"):
            st.markdown(
                "[ã€Œå®šæœŸä¾¿ç•ªå·ã€ã€Œäº‹æ¥­è€…ã€ã®ç™»éŒ²ã¯ã“ã¡ã‚‰ã‹ã‚‰](https://docs.google.com/spreadsheets/d/1Yb-0DLDb-IAKIxDkhaSZxDl-zd2iDHZ3aX3_4mSiQyI/)",
                unsafe_allow_html=True
            )
            st.info("ãƒ‡ãƒ¼ã‚¿ç·¨é›†å¾Œã¯ã€ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹• or ç”»é¢æ›´æ–°ï¼ˆã€ŒF5ã€ã‚­ãƒ¼ï¼‰ã‚’ã—ã¦ãã ã•ã„ã€‚")

        st.markdown('<h2 style="font-size: 24px;">2. ã‚¤ãƒ³ãƒãƒ¼ãƒˆ</h2>', unsafe_allow_html=True)

        with st.expander("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã‚’é–‹ã"):
            st.info("""
                ã“ã¡ã‚‰ã«å¯¾è±¡ã®ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€èª­ã¿è¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚
                â€»ã€Œæ¥½å¤©ã€ã€Œãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã€ã€Œã•ã¨ãµã‚‹åœ¨åº«ã€ã€Œç™¾é¸åœ¨åº«ã€ã¯ä»•æ§˜ä¸Šã€ã“ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å¯¾è±¡å¤–ã§ã™ã€‚
            """)
            filter_col1, filter_col2 = st.columns(2)
            with filter_col1:
                item_codes_to_filter_input = st.text_area(
                    "è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šã§å…¥åŠ›ï¼‰",
                    height=150,
                    key="filter_item_codes"
                )
            with filter_col2:
                vendor_codes_to_filter_input = st.text_area(
                    label="äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ï¼ˆæ”¹è¡ŒåŒºåˆ‡ã‚Šã§å…¥åŠ›ï¼‰", # labelã‚’çŸ­ç¸®
                    height=150,
                    key="filter_vendor_codes",
                    help="â€»ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ3æ–‡å­— or ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ4æ–‡å­— or æ•°å­—2æ–‡å­—+ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ4æ–‡å­—ï¼ˆè¨ˆ6æ–‡å­—ï¼‰" # helpå¼•æ•°ã«èª¬æ˜ã‚’è¿½åŠ 
                )

        # --- ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ ---
        uploaded_files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«åã«ã€Œãƒãƒ¼ã‚¿ãƒ«åã€ã‚’å«ã‚ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n\n(è¤‡æ•°é¸æŠå¯)",
            help="â€»ã€Œãƒãƒ§ã‚¤ã‚¹ã€ã¨ã€Œãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã€ã€ã€Œã•ã¨ãµã‚‹ã€ã¨ã€Œã•ã¨ãµã‚‹åœ¨åº«ã€ã¯ã€ãã‚Œãã‚Œå¿…ãšã‚»ãƒƒãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            type=['csv', 'tsv', 'xlsx', 'txt'],
            accept_multiple_files=True,
            key=f"portal_uploader_{st.session_state.uploader_key}"
        )

        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if uploaded_files:
            uploaded_file_names = {get_sheet_name_from_filename(f.name) for f in uploaded_files}

            # ãƒãƒ§ã‚¤ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢ãƒã‚§ãƒƒã‚¯
            is_choice_present = 'ãƒãƒ§ã‚¤ã‚¹' in uploaded_file_names
            is_choice_stock_present = 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«' in uploaded_file_names
            if is_choice_present ^ is_choice_stock_present:
                st.error("âš ï¸ ã€Œãƒãƒ§ã‚¤ã‚¹ã€ã¨ã€Œãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã€ã¯ã€å¿…ãšã‚»ãƒƒãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

            # ã•ã¨ãµã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢ãƒã‚§ãƒƒã‚¯
            is_sato_present = 'ã•ã¨ãµã‚‹' in uploaded_file_names
            is_sato_stock_present = 'ã•ã¨ãµã‚‹åœ¨åº«' in uploaded_file_names
            if is_sato_present ^ is_sato_stock_present:
                st.error("âš ï¸ ã€Œã•ã¨ãµã‚‹ã€ã¨ã€Œã•ã¨ãµã‚‹åœ¨åº«ã€ã¯ã€å¿…ãšã‚»ãƒƒãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

            # ç™¾é¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢ãƒã‚§ãƒƒã‚¯
            is_hyakusen_present = 'ç™¾é¸' in uploaded_file_names
            is_hyakusen_stock_present = 'ç™¾é¸åœ¨åº«' in uploaded_file_names
            if is_hyakusen_present ^ is_hyakusen_stock_present:
                st.error("âš ï¸ ã€Œç™¾é¸ã€ã¨ã€Œç™¾é¸åœ¨åº«ã€ã¯ã€å¿…ãšã‚»ãƒƒãƒˆã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")

        def show_file_preview(uploaded_file, df_preview, num_rows=5):
            """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹"""
            with st.expander(f"ğŸ“„ **{uploaded_file.name}**"):
                st.dataframe(df_preview.head(num_rows))

        all_uploaded_files = uploaded_files or []

        files_to_process = []   # å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

        # --- ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ ---
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã€ã™ãã«èª­ã¿è¾¼ã‚“ã§å‰å‡¦ç†ã‚’è¡Œã†
        if any(all_uploaded_files):

            # 1. ãƒãƒ¼ã‚¿ãƒ«åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ & ãƒãƒ¼ã‚¿ãƒ«åç‰¹å®šãƒã‚§ãƒƒã‚¯ (äº‹å‰ãƒã‚§ãƒƒã‚¯)
            portal_to_file_map = {} # ã‚­ãƒ¼: sheet_name, å€¤: file.name
            files_to_reject_duplicate = []    # é‡è¤‡æ‹’å¦å¯¾è±¡
            files_to_reject_unknown = []      # ç‰¹å®šä¸å¯æ‹’å¦å¯¾è±¡

            for file in all_uploaded_files:
                if file: # ãƒ•ã‚¡ã‚¤ãƒ«ãŒNoneã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    sheet_name = get_sheet_name_from_filename(file.name)
                    
                    # ãƒãƒ¼ã‚¿ãƒ«åç‰¹å®šä¸å¯ã®å ´åˆ
                    if sheet_name is None:
                         files_to_reject_unknown.append(file.name)
                         continue # ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—

                    if sheet_name in portal_to_file_map:
                        # é‡è¤‡æ¤œå‡º: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ‹’å¦ãƒªã‚¹ãƒˆã¸
                        original_file_name = portal_to_file_map[sheet_name]
                        files_to_reject_duplicate.append((file.name, sheet_name, original_file_name))
                    else:
                        # æ–°è¦ãƒãƒ¼ã‚¿ãƒ«: å‡¦ç†å¯¾è±¡ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        portal_to_file_map[sheet_name] = file.name
                        files_to_process.append(file)

            # 2. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
            
            # (A) ç‰¹å®šã§ããªã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼
            if files_to_reject_unknown:
                error_msg = "âš ï¸ **ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒãƒ¼ã‚¿ãƒ«åã‚’ç‰¹å®šã§ããªã‹ã£ãŸãŸã‚ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚**\n\n"
                error_msg += "ãƒ•ã‚¡ã‚¤ãƒ«åã«æ­£ã—ã„ãƒãƒ¼ã‚¿ãƒ«åï¼ˆä¾‹: æ¥½å¤©, ãƒãƒ§ã‚¤ã‚¹, etc.ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n"
                for f_name in files_to_reject_unknown:
                    error_msg += f"- {f_name}\n"
                st.error(error_msg)

            # (B) é‡è¤‡ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼
            if files_to_reject_duplicate:
                for file_name, portal_name, original_file_name in files_to_reject_duplicate:
                    st.error(f"âš ï¸ **{file_name}** ã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚**'{portal_name}'** ãƒãƒ¼ã‚¿ãƒ«ã¯æ—¢ã« **{original_file_name}** ã«ã‚ˆã£ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚")

            # â˜… ãƒˆãƒ¼ã‚¹ãƒˆè¡¨ç¤ºç”¨ã®ãƒ•ãƒ©ã‚°ã‚’åˆæœŸåŒ–
            new_file_processed = False

            item_codes_list = [code.strip() for code in item_codes_to_filter_input.split('\n') if code.strip()]
            vendor_codes_list = [code.strip() for code in vendor_codes_to_filter_input.split('\n') if code.strip()]

            # 3. å‡¦ç†å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ« (files_to_process) ã®ã¿ãƒ«ãƒ¼ãƒ—å‡¦ç†
            for file in files_to_process:
            
                # fileãŒNoneã®å¯èƒ½æ€§ã¯äº‹å‰ãƒã‚§ãƒƒã‚¯ã§æ’é™¤ã•ã‚Œã¦ã„ã‚‹
                sheet_name = get_sheet_name_from_filename(file.name)
                
                # â˜… å¤‰æ›´: file_id ã®ä»£ã‚ã‚Šã«ã€åå‰ã€ã‚µã‚¤ã‚ºã€ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ„æ€§ã‚’åˆ¤æ–­
                file_key = f"{sheet_name}_metadata"
                current_metadata = (file.name, file.size, file.type)

                # æ—¢ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå¤‰ã‚ã£ã¦ã„ãªã„å ´åˆã¯å†èª­ã¿è¾¼ã¿ã—ãªã„
                # â˜… å¤‰æ›´: file_id ã®æ¯”è¼ƒã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æ¯”è¼ƒã«å¤‰æ›´
                if sheet_name not in st.session_state.dataframes or st.session_state.dataframes.get(file_key) != current_metadata:
                    df = robust_read_file(file)
                    
                    if df is not None:
                        # --- è‹±èªç‰ˆAmazonã®ãƒ˜ãƒƒãƒ€ãƒ¼å¯¾å¿œ ---
                        if sheet_name == 'Amazon':
                            # è‹±èªç‰ˆã®å°æ–‡å­—ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ—¥æœ¬èªãƒ˜ãƒƒãƒ€ãƒ¼ã«ç½®æ›
                            amazon_rename_map = {
                                'sku': 'å‡ºå“è€…SKU',
                                'asin': 'ASIN',
                                'price': 'ä¾¡æ ¼',
                                'quantity': 'æ•°é‡'
                            }
                            # ãƒªãƒãƒ¼ãƒ å®Ÿè¡Œï¼ˆåˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼‰
                            df = df.rename(columns=amazon_rename_map)

                        # --- æ¥½å¤©ã®å‡¦ç† (å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ & ãƒ‡ãƒ¼ã‚¿åŠ å·¥) ---
                        if sheet_name == 'æ¥½å¤©':
                            # 1. å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
                            required_columns = {
                                "å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰", "å•†å“ç•ªå·", "å•†å“å", "å€‰åº«æŒ‡å®š",
                                "ã‚µãƒ¼ãƒè¡¨ç¤º", "è²©å£²æœŸé–“æŒ‡å®šï¼ˆé–‹å§‹æ—¥æ™‚ï¼‰", "è²©å£²æœŸé–“æŒ‡å®šï¼ˆçµ‚äº†æ—¥æ™‚ï¼‰",
                                "æ³¨æ–‡ãƒœã‚¿ãƒ³", "SKUç®¡ç†ç•ªå·", "ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·",
                                "åœ¨åº«æ•°", "SKUå€‰åº«æŒ‡å®š"
                            }
                            missing_cols = required_columns - set(df.columns)
                            if missing_cols:
                                st.error(f"âš ï¸ **{file.name}** ã¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã®å¿…é ˆåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_cols)}")
                                continue  # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸
                            
                            # 2. ãƒ‡ãƒ¼ã‚¿åŠ å·¥: ã€Œã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·ã€ã«å€¤ãŒã‚ã‚‹å ´åˆã®ã¿ã€ã€Œå•†å“ç•ªå·ã€ã«ã‚³ãƒ”ãƒ¼
                            # (ç©ºæ–‡å­—ã§ãªã„å ´åˆã®ã¿ä¸Šæ›¸ãã™ã‚‹)
                            mask_sku = df['ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·'] != ''
                            df.loc[mask_sku, 'å•†å“ç•ªå·'] = df.loc[mask_sku, 'ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·']

                            # 3. ãƒ‡ãƒ¼ã‚¿åŠ å·¥: ã€ŒSKUå€‰åº«æŒ‡å®šã€ã«å€¤ãŒã‚ã‚‹å ´åˆã®ã¿ã€ã€Œå€‰åº«æŒ‡å®šã€ã«ã‚³ãƒ”ãƒ¼
                            # (ç©ºæ–‡å­—ã§ãªã„å ´åˆã®ã¿ä¸Šæ›¸ãã™ã‚‹)
                            mask_warehouse = df['SKUå€‰åº«æŒ‡å®š'] != ''
                            df.loc[mask_warehouse, 'å€‰åº«æŒ‡å®š'] = df.loc[mask_warehouse, 'SKUå€‰åº«æŒ‡å®š']

                            # 4. ãƒ‡ãƒ¼ã‚¿åŠ å·¥: å…ˆé ­è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’åŒã‚°ãƒ«ãƒ¼ãƒ—ã®ä¸‹è¡Œã¸ã‚³ãƒ”ãƒ¼ (groupby + transform first)
                            fill_targets = ['å•†å“å', 'ã‚µãƒ¼ãƒè¡¨ç¤º', 'è²©å£²æœŸé–“æŒ‡å®šï¼ˆé–‹å§‹æ—¥æ™‚ï¼‰', 'è²©å£²æœŸé–“æŒ‡å®šï¼ˆçµ‚äº†æ—¥æ™‚ï¼‰', 'æ³¨æ–‡ãƒœã‚¿ãƒ³']
                            
                            # â˜…é«˜é€ŸåŒ–: groupby().transform('first') ã®ä»£ã‚ã‚Šã« map ã‚’ä½¿ç”¨
                            # å•†å“ç®¡ç†ç•ªå·ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã«ã¤ã„ã¦ã€ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®å…ˆé ­ã®å€¤ã‚’å…¨ä½“ã«é©ç”¨ã™ã‚‹
                            # reset_indexç­‰ã¯ä¸è¦ã€Seriesã¨ã—ã¦å–å¾—
                            grouped_first = df.groupby('å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰')[fill_targets].first()
                            
                            for col in fill_targets:
                                # ãƒãƒƒãƒ”ãƒ³ã‚°å®Ÿè¡Œ (transformã‚ˆã‚Šåœ§å€’çš„ã«é€Ÿã„)
                                df[col] = df['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰'].map(grouped_first[col])
                        # -----------------------------------

                        if sheet_name not in SKIP_FILTERING_SHEETS:
                            df = filter_dataframe(df, sheet_name, item_codes_list, vendor_codes_list)
                        
                        st.session_state.dataframes[sheet_name] = df
                        st.session_state.dataframes[file_key] = current_metadata # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜

                        new_file_processed = True # â˜… æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹

                        # å‰å‡¦ç†ãƒ•ãƒ©ã‚°ã®ãƒªã‚»ãƒƒãƒˆ
                        if sheet_name == 'ãƒãƒ§ã‚¤ã‚¹åœ¨åº«': st.session_state['choice_stock_processed'] = False

            # --- ãƒãƒ§ã‚¤ã‚¹åœ¨åº«ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç† ---
            # ãƒãƒ§ã‚¤ã‚¹ã¨ãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã®ä¸¡æ–¹ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¦ã€ã¾ã å‰å‡¦ç†ãŒã•ã‚Œã¦ã„ãªã„å ´åˆ
            if "ãƒãƒ§ã‚¤ã‚¹" in st.session_state.dataframes and "ãƒãƒ§ã‚¤ã‚¹åœ¨åº«" in st.session_state.dataframes:
                if not st.session_state.get('choice_stock_processed', False):
                    df_choice = st.session_state.dataframes["ãƒãƒ§ã‚¤ã‚¹"]
                    df_choice_stock = st.session_state.dataframes["ãƒãƒ§ã‚¤ã‚¹åœ¨åº«"].copy()
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¿…è¦ãªåˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                    # (ãƒãƒ§ã‚¤ã‚¹: index 1, 102), (ãƒãƒ§ã‚¤ã‚¹åœ¨åº«: index 1)
                    if df_choice.shape[1] > 102 and df_choice_stock.shape[1] > 1:
                        # ãƒãƒ§ã‚¤ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ å•†å“ç®¡ç†ç•ªå·(1) ã¨ è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰(102) ã‚’æŠ½å‡º
                        df_map_source = df_choice[[1, 102]].dropna().copy()
                        # æ–‡å­—åˆ—å‹ã«ã—ã¦å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
                        df_map_source[1] = df_map_source[1].astype(str).str.strip()
                        df_map_source[102] = df_map_source[102].astype(str).str.strip()
                        # å•†å“ç®¡ç†ç•ªå·ã§é‡è¤‡ã‚’é™¤å» (æœ€åˆã®ä¸€ã¤ã‚’æ®‹ã™)
                        df_map_source = df_map_source.drop_duplicates(subset=[1], keep='first')
                        # å•†å“ç®¡ç†ç•ªå·ã‚’ã‚­ãƒ¼ã€è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
                        id_map = df_map_source.set_index(1)[102].to_dict()
                        # ãƒãƒ§ã‚¤ã‚¹åœ¨åº«ã® å•†å“ç®¡ç†ç•ªå·(1) åˆ—ã‚’å–å¾—ã—ã€æ–‡å­—åˆ—å‹ã«å¤‰æ›
                        lookup_keys = df_choice_stock[1].astype(str).str.strip()
                        # mapé–¢æ•°ã‚’ä½¿ã£ã¦è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚’ç´ä»˜ã‘
                        mapped_codes = lookup_keys.map(id_map)
                        # ç´ä»˜ã‘ãŸè¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã‚’å…ˆé ­åˆ—(0åˆ—ç›®)ã«æŒ¿å…¥
                        df_choice_stock.insert(0, 'generated_code', mapped_codes)
                        # åˆ—åã‚’ãƒªã‚»ãƒƒãƒˆ (0, 1, 2, ...)
                        df_choice_stock.columns = range(df_choice_stock.shape[1])
                        # å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                        st.session_state.dataframes["ãƒãƒ§ã‚¤ã‚¹åœ¨åº«"] = df_choice_stock
                        # å‰å‡¦ç†æ¸ˆã¿ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                        st.session_state['choice_stock_processed'] = True

                        # â˜… åœ¨åº«å‡¦ç†ã‚‚æ–°è¦å‡¦ç†ã¨ã¿ãªã™
                        if not new_file_processed: # é‡è¤‡ãƒˆãƒ¼ã‚¹ãƒˆã‚’é¿ã‘ã‚‹
                            new_file_processed = True

            # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ãŒå®Œäº†ã—ãŸå ´åˆã«ãƒˆãƒ¼ã‚¹ãƒˆã‚’è¡¨ç¤º
            if new_file_processed:
                st.toast("ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚", icon="ğŸ“„")

        # --- ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ Expander ---
        # session_state.dataframes ã«ãƒ•ã‚¡ã‚¤ãƒ«IDä»¥å¤–ã®ã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        # _id -> _metadata
        processed_dataframes_exist = any(not k.endswith('_metadata') for k in st.session_state.dataframes)

        # å‡¦ç†æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ Expander ã‚’è¡¨ç¤º
        if processed_dataframes_exist:
            with st.expander("ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                col_idx = 0
                processed_files_count = 0 # æ­£å¸¸ã«å‡¦ç†ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                for file in files_to_process:
                    if file:
                        sheet_name = get_sheet_name_from_filename(file.name)
                        # å‰å‡¦ç†ã®çµæœã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
                        if sheet_name in st.session_state.dataframes:
                            show_file_preview(file, st.session_state.dataframes[sheet_name])
                            processed_files_count += 1
                # ã‚‚ã—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚‹ã®ã«ã€å‡¦ç†ã•ã‚ŒãŸã‚‚ã®ãŒãªã‘ã‚Œã°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
                if processed_files_count == 0 and any(all_uploaded_files):
                    st.write("ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿/å‡¦ç†ã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ãã¾ã›ã‚“ã€‚")
                elif processed_files_count == 0: # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãªã„å ´åˆ
                    st.write("ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

        st.markdown('<h2 style="font-size: 24px;">3. å®Ÿè¡Œ</h2>', unsafe_allow_html=True)

        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«é¸æŠæ©Ÿèƒ½
        st.markdown('<p style="font-size: 14px; margin-top: 10px; margin-bottom: 5px;">é¸æŠã•ã‚ŒãŸãƒãƒ¼ã‚¿ãƒ«ã¨åŸºæº–æ—¥ã‚’å…ƒã«æ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚</p>', unsafe_allow_html=True)

        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒãƒ¼ã‚¿ãƒ«åã®ãƒªã‚¹ãƒˆã‚’å–å¾—
        # _id -> _metadata
        uploaded_portal_names = [p for p in PORTAL_ORDER if p in st.session_state.dataframes and not p.endswith('_metadata')]

        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°
        files_uploaded = bool(uploaded_portal_names)
        
        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã¨æ—¥ä»˜é¸æŠã‚’ã‚«ãƒ©ãƒ ã§æ¨ªä¸¦ã³ã«ã™ã‚‹
        col1, col2 = st.columns([2, 1]) # 2:1 ã®æ¯”ç‡

        with col1:
            if files_uploaded:
                # ã€Œãƒãƒ§ã‚¤ã‚¹ã€ãŒã‚ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
                default_index = 0
                if "ãƒãƒ§ã‚¤ã‚¹" in uploaded_portal_names:
                    default_index = uploaded_portal_names.index("ãƒãƒ§ã‚¤ã‚¹")

                selected_base_portal = st.selectbox(
                    label="ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«é¸æŠ",
                    options=uploaded_portal_names,
                    index=default_index, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠã‚’è¨­å®š
                    label_visibility="collapsed"
                )
            else:
                selected_base_portal = None
                st.selectbox(
                    label="ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«é¸æŠ",
                    options=["ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"],
                    disabled=True,
                    label_visibility="collapsed"
                )

        # æ—¥ä»˜é¸æŠã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ (ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼)
        with col2:
            selected_date = st.date_input(
                label="åŸºæº–æ—¥", # ãƒ©ãƒ™ãƒ«ã¯éè¡¨ç¤º
                value=TODAY, # L22 ã§å®šç¾©ã—ãŸæœ¬æ—¥æ—¥ä»˜
                disabled=not files_uploaded,
                label_visibility="collapsed",
                help="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šã®åŸºæº–ã¨ãªã‚‹æ—¥ä»˜ã‚’é¸æŠã—ã¾ã™ã€‚"
            )

        # --- ã€Œæ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤ºã€ãƒœã‚¿ãƒ³ ---
        st.markdown('<div class="button-container" style="margin-top: 10px;">', unsafe_allow_html=True)
        
        # â˜… å®Ÿè¡Œä¸­ãƒ•ãƒ©ã‚°ã®åˆæœŸåŒ–
        if 'is_running' not in st.session_state:
            st.session_state.is_running = False

        # â˜… ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        def start_processing():
            st.session_state.is_running = True

        run_button = st.button(
            "æ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤º",
            key="sidebar_run_button",
            disabled=not files_uploaded or st.session_state.is_running, # â˜… ãƒ•ã‚¡ã‚¤ãƒ«æœªé¸æŠ ã¾ãŸã¯ å®Ÿè¡Œä¸­ã¯ç„¡åŠ¹åŒ–
            on_click=start_processing # â˜… ã‚¯ãƒªãƒƒã‚¯æ™‚ã«å‡¦ç†é–‹å§‹ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
        )
        st.markdown('</div>', unsafe_allow_html=True)


    # --- ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸UIã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
    # ãƒœã‚¿ãƒ³ã®æˆ»ã‚Šå€¤ã§ã¯ãªãã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®ãƒ•ãƒ©ã‚°ã§åˆ¤å®š
    if st.session_state.is_running:
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒæ­£å¸¸ã‹ãƒã‚§ãƒƒã‚¯
        if sheets_service is None:
            st.error("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ¥ç¶šã§ãã¾ã›ã‚“ã€‚èªè¨¼è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.session_state.is_running = False # åœæ­¢ã™ã‚‹å‰ã«ãƒ•ãƒ©ã‚°ã‚’æˆ»ã™
            st.stop()
            
        # å‡¦ç†å®Ÿè¡Œå‰ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
        # _id -> _metadata
        loaded_df_names = {k for k in st.session_state.dataframes if not k.endswith('_metadata')}
        
        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹
        if selected_base_portal is None:
            st.error("ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‚’é¸æŠã§ãã¾ã›ã‚“ã€‚")

        # ã•ã¨ãµã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        is_sato_present = 'ã•ã¨ãµã‚‹' in loaded_df_names
        is_sato_stock_present = 'ã•ã¨ãµã‚‹åœ¨åº«' in loaded_df_names
        satofuru_files_ok = not (is_sato_present ^ is_sato_stock_present)
        if not satofuru_files_ok:
            st.error("ã€Œã•ã¨ãµã‚‹ã€ã¨ã€Œã•ã¨ãµã‚‹åœ¨åº«ã€ã¯ä¸¡æ–¹åŒæ™‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # ç™¾é¸ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒšã‚¢å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        is_hyakusen_present = 'ç™¾é¸' in loaded_df_names
        is_hyakusen_stock_present = 'ç™¾é¸åœ¨åº«' in loaded_df_names
        hyakusen_files_ok = not (is_hyakusen_present ^ is_hyakusen_stock_present)
        if not hyakusen_files_ok:
             st.error("ã€Œç™¾é¸ã€ã¨ã€Œç™¾é¸åœ¨åº«ã€ã¯ä¸¡æ–¹åŒæ™‚ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ãŒNoneã§ãªã„ã“ã¨ã¨ã€ã•ã¨ãµã‚‹ãƒ»ç™¾é¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒOKãªã“ã¨ã‚’ç¢ºèª
        if selected_base_portal and satofuru_files_ok and hyakusen_files_ok:
            
            # é¸æŠã•ã‚ŒãŸæ—¥ä»˜ã‚’ 'YYYYMMDD' å½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›
            select_date_str = selected_date.strftime('%Y%m%d')
            
            # â˜… è¿½åŠ : ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«åŸºæº–æ—¥ã¨ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‚’ä¿å­˜
            st.session_state.current_select_date_str = select_date_str
            st.session_state.current_base_portal = selected_base_portal

            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨ˆç®—ä¸­..."):
                try:
                    teiki_bin_codes = get_teiki_data_from_gsheet(sheets_service)
                    if teiki_bin_codes is None: # å–å¾—å¤±æ•—
                        st.error("å®šæœŸä¾¿DBï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.session_state.is_running = False # â˜… åœæ­¢ã™ã‚‹å‰ã«ãƒ•ãƒ©ã‚°ã‚’æˆ»ã™
                        st.stop()
                    
                    # â˜… å•†å“ç®¡ç†DBã®èª­ã¿è¾¼ã¿ã‚’å‰Šé™¤
                    
                    df_business = get_business_data_from_gsheet(sheets_service)
                    if df_business is None: # å–å¾—å¤±æ•—
                        st.error("äº‹æ¥­è€…DBï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆï¼‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                        st.session_state.is_running = False # â˜… åœæ­¢ã™ã‚‹å‰ã«ãƒ•ãƒ©ã‚°ã‚’æˆ»ã™
                        st.stop()
                    # ---------------------------------

                    # â˜… å¤‰æ›´: _id -> _metadata
                    full_data = {k: v for k, v in st.session_state.dataframes.items() if not k.endswith('_metadata')}
                    
                    master_items = {}
                    base_portal_name = selected_base_portal
                    df_base = full_data.get(base_portal_name)
                    
                    # ãƒ™ãƒ¼ã‚¹ãƒãƒ¼ã‚¿ãƒ«ã‹ã‚‰è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ã¨åç§°ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                    df_base_data = df_base # robust_read_fileã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿
                    
                    if df_base_data is not None:
                        code_col = KEY_COLUMN_MAP.get(base_portal_name)
                        name_col = PORTAL_NAME_COLUMN_MAP.get(base_portal_name)

                        if code_col is not None:
                            # gspread ã¨ googleapiclient ã§ .dropna() ã®æŒ™å‹•ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚
                            # ã‚­ãƒ¼åˆ—ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ subset ã‚’æŒ‡å®šã™ã‚‹
                            subset_col = [code_col] if code_col in df_base_data.columns or isinstance(code_col, int) else None
                            if subset_col:
                                df_master_source = df_base_data.dropna(subset=subset_col).copy()
                            else:
                                df_master_source = df_base_data.copy() # subset ãªã— (ä¸‡ãŒä¸€ã®å ´åˆ)

                            
                            # --- ã‚­ãƒ¼åˆ—ã®å‹ï¼ˆint or strï¼‰ã§å‡¦ç†ã‚’åˆ†å² ---
                            if isinstance(code_col, int):
                                # (ãƒãƒ§ã‚¤ã‚¹ç³»: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã§å‚ç…§)
                                # (lookup_mapså´ã¨ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã‚’åˆã‚ã›ã‚‹)
                                # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                df_master_source['key'] = df_master_source[code_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                            
                            elif isinstance(code_col, str):
                                # (ãã®ä»–: ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
                                if base_portal_name == 'ã•ã¨ãµã‚‹':
                                    # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                    df_master_source['key'] = df_master_source[code_col].astype(str).str.extract(r'\[(.*?)\]', expand=False).fillna('').str.upper()
                                else:
                                    # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                    df_master_source['key'] = df_master_source[code_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                            # é‡è¤‡ã‚’é™¤å»
                            unique_items = df_master_source[df_master_source['key'] != ''].drop_duplicates(subset=['key'], keep='first')

                            # ãƒã‚¹ã‚¿ãƒ¼è¾æ›¸ã‚’ä½œæˆ
                            for _, row in unique_items.iterrows():
                                item_code = row['key']
                                item_name = ""
                                if name_col is not None:
                                    try:
                                        item_name = str(row[name_col]).strip()
                                    except KeyError:
                                        item_name = ""
                                master_items[item_code] = item_name
                        
                    lookup_maps, parent_lookup_maps = {}, {}

                    # --- æ¥½å¤©ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ ---
                    # â˜… å•†å“ç®¡ç†DBå»ƒæ­¢ã«ä¼´ã„ã€memo_map ã‚‚å»ƒæ­¢ï¼ˆç©ºè¾æ›¸ã¨ã™ã‚‹ï¼‰
                    memo_map = {}

                    # æ¥½å¤©ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„ç¨®å¯¾å¿œè¾æ›¸ã‚’ä½œæˆ (ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
                    rakuten_product_id_map = {} # å•†å“ç•ªå· -> è¡Œãƒ‡ãƒ¼ã‚¿
                    rakuten_management_id_map = {} # å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰ -> è¡Œãƒ‡ãƒ¼ã‚¿
                    rakuten_sku_code_map = {} # SKUç®¡ç†ç•ªå· -> è¡Œãƒ‡ãƒ¼ã‚¿
                    
                    # â˜… ã€è¿½åŠ ã€‘æ¥½å¤©ã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒãƒƒãƒ—ä½œæˆ (å•†å“ç®¡ç†ç•ªå· -> è¡Œãƒªã‚¹ãƒˆ)
                    rakuten_group_map = {}

                    if 'æ¥½å¤©' in full_data:
                        df_rakuten = full_data['æ¥½å¤©']
                        # robust_read_fileã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿ã®ãŸã‚ã€iloc[1:] ã¯ä¸è¦
                        df_rakuten_data = df_rakuten
                        
                        # â˜…ã€Œå•†å“ç•ªå·ã€åˆ—ã®ã‚½ãƒ¼ãƒˆ -> è¡Œãƒ‡ãƒ¼ã‚¿
                        if 'å•†å“ç•ªå·' in df_rakuten_data.columns:
                            # ã¾ãšå•†å“ç•ªå·ãŒã‚ã‚‹è¡Œã‚’æŠ½å‡º
                            df_rakuten_b = df_rakuten_data.dropna(subset=['å•†å“ç•ªå·']).copy()
                            
                            # --- åˆ†å‰²ã‚½ãƒ¼ãƒˆ ---

                            # 1. SKUæœ‰ç„¡ã®åˆ¤å®šï¼ˆãƒ•ãƒ©ã‚°ä½œæˆï¼‰
                            # ã€Œã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·ã€ã«å€¤ãŒã‚ã‚‹ï¼ˆç©ºæ–‡å­—ã§ãªã„ï¼‰å ´åˆã¯True
                            if 'ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·' in df_rakuten_b.columns:
                                has_sku_mask = (df_rakuten_b['ã‚·ã‚¹ãƒ†ãƒ é€£æºç”¨SKUç•ªå·'].astype(str).str.strip() != '')
                            else:
                                has_sku_mask = pd.Series(False, index=df_rakuten_b.index)

                            # --- ãƒ©ãƒ³ã‚¯è¨ˆç®—ç”¨ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå…¨è¡Œã«å¯¾ã—ã¦è¨ˆç®—ã ã‘è¡Œã†ï¼‰ ---
                            # â€»è¨ˆç®—è‡ªä½“ã¯å…¨è¡Œã«è¡Œã†ãŒã€ã‚½ãƒ¼ãƒˆã«ä½¿ã†ã®ã¯SKUã‚ã‚Šã®è¡Œã ã‘ã«ã™ã‚‹

                            # æ—¥ä»˜å‡¦ç†ç”¨ã®é–¢æ•°
                            def _get_date_str(x):
                                s = str(x).strip()
                                return re.sub(r'[^0-9]', '', s)[:8]

                            # åˆ—åã®å®šç¾©
                            col_warehouse = 'å€‰åº«æŒ‡å®š' if 'å€‰åº«æŒ‡å®š' in df_rakuten_b.columns else None
                            col_search = 'ã‚µãƒ¼ãƒè¡¨ç¤º' if 'ã‚µãƒ¼ãƒè¡¨ç¤º' in df_rakuten_b.columns else None
                            col_order = 'æ³¨æ–‡ãƒœã‚¿ãƒ³' if 'æ³¨æ–‡ãƒœã‚¿ãƒ³' in df_rakuten_b.columns else None
                            col_start = 'è²©å£²æœŸé–“æŒ‡å®šï¼ˆé–‹å§‹æ—¥æ™‚ï¼‰' if 'è²©å£²æœŸé–“æŒ‡å®šï¼ˆé–‹å§‹æ—¥æ™‚ï¼‰' in df_rakuten_b.columns else None
                            col_end = 'è²©å£²æœŸé–“æŒ‡å®šï¼ˆçµ‚äº†æ—¥æ™‚ï¼‰' if 'è²©å£²æœŸé–“æŒ‡å®šï¼ˆçµ‚äº†æ—¥æ™‚ï¼‰' in df_rakuten_b.columns else None
                            col_stock = 'åœ¨åº«æ•°' if 'åœ¨åº«æ•°' in df_rakuten_b.columns else None
                            
                            current_date_str = TODAY_STR

                            # ã€1ã€‘ã€Œå€‰åº«æŒ‡å®šã€: '0' ãŒå„ªå…ˆ -> æ˜‡é †
                            def _calc_warehouse_rank(x):
                                if col_warehouse and str(x).strip() == '0': return 0
                                return 1
                            df_rakuten_b['p_rank_1'] = df_rakuten_b[col_warehouse].apply(_calc_warehouse_rank) if col_warehouse else 1

                            # ã€2ã€‘ã€Œã‚µãƒ¼ãƒè¡¨ç¤ºã€: '1' ãŒå„ªå…ˆ -> é™é †
                            df_rakuten_b['p_rank_2'] = pd.to_numeric(df_rakuten_b[col_search], errors='coerce').fillna(0) if col_search else 0

                            # ã€3ã€‘ã€Œæ³¨æ–‡ãƒœã‚¿ãƒ³ã€: '1' ãŒå„ªå…ˆ -> é™é †
                            df_rakuten_b['p_rank_3'] = pd.to_numeric(df_rakuten_b[col_order], errors='coerce').fillna(0) if col_order else 0

                            # ã€4ã€‘ã€Œé–‹å§‹æ—¥æ™‚ã€
                            s_start_dates = df_rakuten_b[col_start].apply(_get_date_str) if col_start else pd.Series('', index=df_rakuten_b.index)
                            def _calc_start_cat(d):
                                if not d: return 0      # â‘  ç©º
                                if d <= current_date_str: return 1 # â‘¡ éå»
                                return 2                # â‘¢ æœªæ¥
                            df_rakuten_b['p_rank_4_cat'] = s_start_dates.apply(_calc_start_cat)
                            df_rakuten_b['p_rank_4_val'] = s_start_dates

                            # ã€5ã€‘ã€Œçµ‚äº†æ—¥æ™‚ã€
                            s_end_dates = df_rakuten_b[col_end].apply(_get_date_str) if col_end else pd.Series('', index=df_rakuten_b.index)
                            def _calc_end_cat(d):
                                if not d: return 0      # â‘  ç©º
                                if d >= current_date_str: return 1 # â‘¡ æœªæ¥
                                return 2                # â‘¢ éå»
                            df_rakuten_b['p_rank_5_cat'] = s_end_dates.apply(_calc_end_cat)
                            df_rakuten_b['p_rank_5_val'] = s_end_dates

                            # ã€6ã€‘ã€Œåœ¨åº«æ•°ã€: å¤šã„æ–¹ãŒå„ªå…ˆ -> é™é †
                            df_rakuten_b['p_rank_6'] = pd.to_numeric(df_rakuten_b[col_stock], errors='coerce').fillna(0) if col_stock else 0

                            # --- ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²ã¨ã‚½ãƒ¼ãƒˆ ---
                            
                            # SKUã‚ã‚Šã®ã‚°ãƒ«ãƒ¼ãƒ—
                            df_sku = df_rakuten_b[has_sku_mask].copy()
                            # SKUãªã—ã®ã‚°ãƒ«ãƒ¼ãƒ—
                            df_no_sku = df_rakuten_b[~has_sku_mask].copy()

                            # SKUã‚ã‚Šã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ã€å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹
                            if not df_sku.empty:
                                sort_columns = [
                                    'p_rank_1',     # ã€1ã€‘å€‰åº« (0å„ªå…ˆ -> æ˜‡é †)
                                    'p_rank_2',     # ã€2ã€‘ã‚µãƒ¼ãƒ (1å„ªå…ˆ -> é™é †)
                                    'p_rank_3',     # ã€3ã€‘æ³¨æ–‡ (1å„ªå…ˆ -> é™é †)
                                    'p_rank_4_cat', # ã€4ã€‘é–‹å§‹åŒºåˆ† (ç©º<éå»<æœªæ¥ -> æ˜‡é †)
                                    'p_rank_4_val', # ã€4ã€‘é–‹å§‹æ—¥å€¤ (å¤ã„æ—¥ä»˜å„ªå…ˆ -> æ˜‡é †)
                                    'p_rank_5_cat', # ã€5ã€‘çµ‚äº†åŒºåˆ† (ç©º<æœªæ¥<éå» -> æ˜‡é †)
                                    'p_rank_5_val', # ã€5ã€‘çµ‚äº†æ—¥å€¤ (æ–°ã—ã„æ—¥ä»˜å„ªå…ˆ -> é™é †)
                                    'p_rank_6'      # ã€6ã€‘åœ¨åº« (å¤šã„é † -> é™é †)
                                ]
                                asc_settings = [True, False, False, True, True, True, False, False]
                                df_sku = df_sku.sort_values(by=sort_columns, ascending=asc_settings)
                            
                            # SKUãªã—ã‚°ãƒ«ãƒ¼ãƒ—ã¯ã‚½ãƒ¼ãƒˆã—ãªã„ï¼ˆå…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«é †åºã‚’ç¶­æŒï¼‰
                            # ä½•ã‚‚ã—ãªã„

                            # çµåˆï¼ˆSKUã‚ã‚Šã‚’ä¸Šã«ï¼‰
                            df_rakuten_b = pd.concat([df_sku, df_no_sku])

                            # é‡è¤‡æ’é™¤ (keep='first'ãªã®ã§ã€SKUã‚ã‚ŠãŒå„ªå…ˆã•ã‚Œã€åŒã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã¯ã‚½ãƒ¼ãƒˆä¸Šä½/ãƒ•ã‚¡ã‚¤ãƒ«ä¸Šä½ãŒæ®‹ã‚‹)
                            df_rakuten_b = df_rakuten_b.drop_duplicates(subset=['å•†å“ç•ªå·'], keep='first')
                            
                            # è¾æ›¸åŒ–
                            rakuten_product_id_map = {str(row['å•†å“ç•ªå·']).strip().upper(): row.to_dict() for _, row in df_rakuten_b.iterrows()}
                        
                        # Aåˆ—(å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰) -> è¡Œãƒ‡ãƒ¼ã‚¿
                        if 'å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰' in df_rakuten_data.columns:
                            df_rakuten_a = df_rakuten_data.dropna(subset=['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰']).drop_duplicates(subset=['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰'], keep='first')
                            # â˜… å¤‰æ›´: .upper() ã«çµ±ä¸€
                            rakuten_management_id_map = {str(row['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰']).strip().upper(): row.to_dict() for _, row in df_rakuten_a.iterrows()}

                        # Håˆ—(SKUç®¡ç†ç•ªå·) -> è¡Œãƒ‡ãƒ¼ã‚¿
                        if 'SKUç®¡ç†ç•ªå·' in df_rakuten_data.columns:
                            df_rakuten_h = df_rakuten_data.dropna(subset=['SKUç®¡ç†ç•ªå·']).drop_duplicates(subset=['SKUç®¡ç†ç•ªå·'], keep='first')
                            # â˜… é‡è¦: SKUç®¡ç†ç•ªå·ã¯å³å¯†æ¯”è¼ƒã®ãŸã‚ã€.upper() ã—ãªã„ (å…ƒã®ã¾ã¾)
                            rakuten_sku_code_map = {str(row['SKUç®¡ç†ç•ªå·']).strip(): row.to_dict() for _, row in df_rakuten_h.iterrows()}
                        
                        # â˜… ã€è¿½åŠ ã€‘ã‚°ãƒ«ãƒ¼ãƒ—ãƒãƒƒãƒ—ã®æ§‹ç¯‰
                        if 'å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰' in df_rakuten_data.columns:
                            for _, row in df_rakuten_data.iterrows():
                                mid = str(row['å•†å“ç®¡ç†ç•ªå·ï¼ˆå•†å“URLï¼‰']).strip().upper()
                                if mid:
                                    if mid not in rakuten_group_map: rakuten_group_map[mid] = []
                                    rakuten_group_map[mid].append(row.to_dict())
                    
                    # --- ä»–ãƒãƒ¼ã‚¿ãƒ«ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ (lookup_maps ä½œæˆ) ---
                    for name, df in full_data.items():
                        key_col = KEY_COLUMN_MAP.get(name)
                        
                        if key_col is None:
                            continue # ã‚­ãƒ¼åˆ—ãŒæœªå®šç¾©ã®ã‚·ãƒ¼ãƒˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        
                        df_data_only = df # robust_read_fileã§ãƒ˜ãƒƒãƒ€ãƒ¼å‡¦ç†æ¸ˆã¿
                        
                        # --- ã‚­ãƒ¼åˆ—ã®å‹ï¼ˆint or strï¼‰ã§å‡¦ç†ã‚’åˆ†å² ---
                        if isinstance(key_col, int):
                            # (ãƒãƒ§ã‚¤ã‚¹ç³»: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã§å‚ç…§)
                            if df.shape[1] <= key_col:
                                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{name}' ã®åˆ—æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã‚­ãƒ¼åˆ— {key_col} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                                continue
                                
                            df_cleaned = df_data_only.dropna(subset=[key_col]).copy()
                            # BOMç­‰ã®é™¤å»ã€.0é™¤å»ã€ç©ºç™½é™¤å»
                            # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                            df_cleaned['key_col_str'] = df_cleaned[key_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                            df_cleaned = df_cleaned[df_cleaned['key_col_str'] != '']
                            
                            unique_data = df_cleaned.drop_duplicates(subset=['key_col_str'], keep='first')
                            # ã‚­ãƒ¼ãŒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·(0, 1...)ã®è¾æ›¸ã‚’ä½œæˆ
                            lookup_maps[name] = {row['key_col_str']: row.to_dict() for _, row in unique_data.iterrows()}

                        elif isinstance(key_col, str):
                            # (ãã®ä»–: ãƒ˜ãƒƒãƒ€ãƒ¼åã§å‚ç…§)
                            if key_col not in df.columns:
                                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ« '{name}' ã«å¿…è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ '{key_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                                continue
                            
                            df_cleaned = df_data_only.dropna(subset=[key_col]).copy()
                            
                            if name == 'ã•ã¨ãµã‚‹':
                                temp_map = {}
                                for _, row in df_cleaned.iterrows():
                                    # 'ãŠç¤¼å“å' åˆ—(key_col)ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                                    match = re.search(r'\[(.*?)\]', str(row.get(key_col, ''))) 
                                    if match:
                                        # â˜… å¤‰æ›´: ã™ã¹ã¦ .upper() ã«çµ±ä¸€
                                        key = match.group(1).strip().upper()
                                        if key and key not in temp_map:
                                            # ã‚­ãƒ¼ãŒãƒ˜ãƒƒãƒ€ãƒ¼å('ãŠç¤¼å“ID', 'ãŠç¤¼å“å'...)ã®è¾æ›¸ã‚’ä½œæˆ
                                            temp_map[key] = row.to_dict()
                                lookup_maps[name] = temp_map
                            else:
                                # BOMç­‰ã®é™¤å»ã€.0é™¤å»ã€ç©ºç™½é™¤å»
                                # â˜… å¤‰æ›´: ã™ã¹ã¦ .str.upper() ã«çµ±ä¸€
                                df_cleaned['key_col_str'] = df_cleaned[key_col].astype(str).str.replace('\ufeff', '', regex=False).str.replace(r'\.0$', '', regex=True).str.strip().str.upper()
                                df_cleaned = df_cleaned[df_cleaned['key_col_str'] != '']
                                
                                unique_data = df_cleaned.drop_duplicates(subset=['key_col_str'], keep='first')
                                # ã‚­ãƒ¼ãŒãƒ˜ãƒƒãƒ€ãƒ¼å('å•†å“ç•ªå·', 'å•†å“å'...)ã®è¾æ›¸ã‚’ä½œæˆ
                                lookup_maps[name] = {row['key_col_str']: row.to_dict() for _, row in unique_data.iterrows()}

                    results_data = []
                    uploaded_portals = [p for p in PORTAL_ORDER if p in full_data]

                    for code, name in master_items.items():
                        statuses = {
                            portal: calculate_status(
                                portal, code, lookup_maps, parent_lookup_maps,
                                
                                # åŸºæº–æ—¥(æ–‡å­—åˆ—)ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã¨ã—ã¦æ¸¡ã™
                                select_date_str=select_date_str,
                                
                                # æ¥½å¤©ç”¨ã®è¾æ›¸ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã¨ã—ã¦æ¸¡ã™
                                memo_map=memo_map,
                                rakuten_product_id_map=rakuten_product_id_map,
                                rakuten_management_id_map=rakuten_management_id_map,
                                rakuten_sku_code_map=rakuten_sku_code_map,
                                # â˜… ã€è¿½åŠ ã€‘æ¥½å¤©ã®ã‚°ãƒ«ãƒ¼ãƒ—ãƒãƒƒãƒ—ã‚’æ¸¡ã™
                                rakuten_group_map=rakuten_group_map
                            ) for portal in uploaded_portals
                        }
                        
                        status_values = list(statuses.values())
                        
                        # --- ãƒã‚§ãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯ ---
                        unique_statuses = set(status_values)
                        
                        # ã€Œéè¡¨ç¤ºã€ã€Œåœ¨åº«0ã€ã€Œå—ä»˜çµ‚äº†ã€ã€Œå€‰åº«ã€ã®4ç¨®ã‚’ã€Œã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ã€ã¨å®šç¾©
                        allowed_gray_statuses = {'éè¡¨ç¤º', 'åœ¨åº«0', 'å—ä»˜çµ‚äº†', 'å€‰åº«'}
                        
                        # ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ä»¥å¤–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆå…¬é–‹ä¸­ã€æœªç™»éŒ²ãªã©ï¼‰ã‚’æŠ½å‡º
                        main_statuses = unique_statuses - allowed_gray_statuses
                        
                        # ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æŠ½å‡º
                        gray_statuses = unique_statuses.intersection(allowed_gray_statuses)
                        
                        check_val = "OK" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’OKã«è¨­å®š
                        
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ä»¥å¤–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ2ç¨®é¡ä»¥ä¸Šã‚ã‚‹å ´åˆ (ä¾‹: 'å…¬é–‹ä¸­'ã¨'æœªç™»éŒ²')
                        if len(main_statuses) >= 2:
                            check_val = "è¦ç¢ºèª"
                        
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ä»¥å¤–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ1ç¨®é¡ã‚ã‚Šã€ã‹ã¤ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚‚1ç¨®é¡ä»¥ä¸Šã‚ã‚‹å ´åˆ (ä¾‹: 'å…¬é–‹ä¸­'ã¨'åœ¨åº«0')
                        elif len(main_statuses) == 1 and len(gray_statuses) >= 1:
                            check_val = "è¦ç¢ºèª"
                        
                        public_count = sum(1 for s in status_values if s == 'å…¬é–‹ä¸­')
                        
                        teiki_bin_flag = 'ã€‡' if code in teiki_bin_codes else 'Ã—'
                            
                        result_row = {'è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰': code, 'è¿”ç¤¼å“å': name, 'äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰': generate_vendor_code(code), **statuses,
                                      'ãƒã‚§ãƒƒã‚¯': check_val, 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°': teiki_bin_flag, 'å…¬é–‹ä¸­ã®æ•°': public_count}
                        results_data.append(result_row)
                    
                    if results_data:
                        df_results = pd.DataFrame(results_data)
                        
                        # df_business ã¯ Gsheetã‹ã‚‰å–å¾—æ¸ˆã¿ã®ã‚‚ã®ã‚’ä½¿ç”¨
                        if not df_business.empty:
                            df_business_names = df_business[['äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­è€…å']]
                            df_results = pd.merge(df_results, df_business_names, on='äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰', how='left')
                            df_results['äº‹æ¥­è€…å'] = df_results['äº‹æ¥­è€…å'].fillna('')
                        else:
                            df_results['äº‹æ¥­è€…å'] = ''
                        
                        base_columns = ['è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰', 'è¿”ç¤¼å“å', 'äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰', 'äº‹æ¥­è€…å']
                        base_portal_column_list = [base_portal_name] if base_portal_name in df_results.columns else []
                        other_portal_columns = [
                            p for p in PORTAL_ORDER 
                            if p in df_results.columns and p != base_portal_name
                        ]
                        utility_columns = ['ãƒã‚§ãƒƒã‚¯', 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°', 'å…¬é–‹ä¸­ã®æ•°']
                        display_columns = base_columns + base_portal_column_list + other_portal_columns + utility_columns
                        final_display_columns = [col for col in display_columns if col in df_results.columns]
                        st.session_state.results_df = df_results.reindex(columns=final_display_columns)
                    else:
                        st.session_state.results_df = pd.DataFrame()
                    
                except Exception as e:
                    st.error(f"å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"); import traceback; st.code(traceback.format_exc())
                    st.session_state.results_df = pd.DataFrame()

            # â˜… è¿½åŠ : å‡¦ç†å®Œäº†ã®ãƒˆãƒ¼ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            st.toast("æ²è¼‰çŠ¶æ³ã®è¡¨ç¤ºã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚", icon="ğŸ“Š")
            
            # â˜… å‡¦ç†å®Œäº†å¾Œã«ãƒ•ãƒ©ã‚°ã‚’ä¸‹ã‚ã—ã¦å†å®Ÿè¡Œï¼ˆãƒœã‚¿ãƒ³ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ãŸã‚ï¼‰
            st.session_state.is_running = False
            st.rerun()

        else:
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ãƒ©ã‚°ã ã‘ä¸‹ã‚ã—ã¦rerunã—ãªã„ï¼ˆã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã•ã›ãŸã¾ã¾ã«ã™ã‚‹ï¼‰
            st.session_state.is_running = False

    st.markdown('<h2 style="font-size: 26px;">3. æ²è¼‰çŠ¶æ³</h2>', unsafe_allow_html=True)

    # ãƒªã‚»ãƒƒãƒˆå®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    if 'show_reset_success' in st.session_state:
        st.toast("æ²è¼‰çŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚", icon="âœ…")
        del st.session_state.show_reset_success

    if st.session_state.results_df.empty:
        if run_button:
            st.warning("è¡¨ç¤ºå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            st.info("ãƒ•ã‚¡ã‚¤ãƒ«ã‚„DBã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰è¨­å®šã—ã€ã€Œæ²è¼‰çŠ¶æ³ã‚’è¡¨ç¤ºã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    else:
        df_to_display = st.session_state.results_df.copy()
        
        # --- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚»ã‚¯ã‚·ãƒ§ãƒ³ ---
        filter_cols = st.columns(4)

        # â˜… ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (session_stateã«ä¿å­˜ã™ã‚‹ãŸã‚)
        def update_f_search(): st.session_state.f_search = st.session_state.w_search
        def update_f_vendor(): st.session_state.f_vendor = st.session_state.w_vendor
        def update_f_check(): st.session_state.f_check = st.session_state.w_check
        def update_f_teiki(): st.session_state.f_teiki = st.session_state.w_teiki

        with filter_cols[0]:
            # å…¨æ–‡æ¤œç´¢: session_state.f_search ã®å€¤ã‚’ä½¿ç”¨
            st.text_input(
                "å…¨æ–‡æ¤œç´¢ (ã‚³ãƒ¼ãƒ‰/è¿”ç¤¼å“å/äº‹æ¥­è€…å):",
                value=st.session_state.f_search,
                key="w_search",
                on_change=update_f_search
            )
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            if st.session_state.f_search:
                search_text = st.session_state.f_search
                df_to_display = df_to_display[
                    df_to_display['è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰'].str.contains(search_text, na=False, case=False) |
                    df_to_display['è¿”ç¤¼å“å'].str.contains(search_text, na=False, case=False) |
                    df_to_display['äº‹æ¥­è€…å'].str.contains(search_text, na=False, case=False)
                ]

        with filter_cols[1]:
            vendor_list = sorted(st.session_state.results_df['äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰'].unique())
            vendor_options = ["ã™ã¹ã¦"] + vendor_list
            
            # é¸æŠè‚¢ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
            current_vendor = st.session_state.f_vendor
            v_index = vendor_options.index(current_vendor) if current_vendor in vendor_options else 0

            st.selectbox(
                "äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰:",
                vendor_options,
                index=v_index,
                key="w_vendor",
                on_change=update_f_vendor
            )
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            if st.session_state.f_vendor != "ã™ã¹ã¦":
                df_to_display = df_to_display[df_to_display['äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰'] == st.session_state.f_vendor]

        with filter_cols[2]:
            check_options = ["ã™ã¹ã¦", "OK", "è¦ç¢ºèª"]
            current_check = st.session_state.f_check
            c_index = check_options.index(current_check) if current_check in check_options else 0
            
            st.selectbox(
                "ãƒã‚§ãƒƒã‚¯:",
                check_options,
                index=c_index,
                key="w_check",
                on_change=update_f_check
            )
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            if st.session_state.f_check != "ã™ã¹ã¦":
                df_to_display = df_to_display[df_to_display['ãƒã‚§ãƒƒã‚¯'] == st.session_state.f_check]

        with filter_cols[3]:
            teiki_options = ["ã™ã¹ã¦", "ã€‡", "Ã—"]
            current_teiki = st.session_state.f_teiki
            t_index = teiki_options.index(current_teiki) if current_teiki in teiki_options else 0
            
            st.selectbox(
                "å®šæœŸä¾¿:",
                teiki_options,
                index=t_index,
                key="w_teiki",
                on_change=update_f_teiki
            )
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨
            if st.session_state.f_teiki != "ã™ã¹ã¦":
                df_to_display = df_to_display[df_to_display['å®šæœŸä¾¿ãƒ•ãƒ©ã‚°'] == st.session_state.f_teiki]
        
        # --- ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š (â˜… DataFrameæç”»å‰ã«è¨ˆç®—å‡¦ç†ã‚’ç§»å‹• â˜…) ---
        # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®è¡¨ç¤ºä»¶æ•°
        ITEMS_PER_PAGE = 500 
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã®ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’è¨ˆç®—
        total_items = len(df_to_display)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœã«åŸºã¥ãã€ç·ãƒšãƒ¼ã‚¸æ•°ã¨ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’è¨ˆç®—ãƒ»è£œæ­£
        if total_items > 0:
            # ç·ãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—
            total_pages = (total_items // ITEMS_PER_PAGE) + (1 if total_items % ITEMS_PER_PAGE > 0 else 0)
            
            # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å–å¾—
            current_page = st.session_state.current_page
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨å¾Œã« total_pages ãŒæ¸›ã£ãŸå ´åˆã€ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ãŒæœ€å¤§ãƒšãƒ¼ã‚¸ã‚’è¶…ãˆãªã„ã‚ˆã†ã«è£œæ­£
            if current_page > total_pages:
                st.session_state.current_page = total_pages
                current_page = total_pages # ã‚¹ãƒ©ã‚¤ã‚¹å‡¦ç†ç”¨ã«ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‚‚æ›´æ–°
        else:
            # ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®å ´åˆ
            total_pages = 1
            st.session_state.current_page = 1
            current_page = 1
        
        st.write("")

        # è¡¨ç¤ºä»¶æ•°ã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ã‚’æ¨ªä¸¦ã³ã«é…ç½®
        count_col, _, button_col = st.columns([3, 6, 4]) 

        with count_col:
            # èª­ã¿å–ã£ãŸã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã®ä»¶æ•°
            total_count = len(st.session_state.results_df)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ã‹ã‘ãŸçŠ¶æ…‹ã®ä»¶æ•°
            filtered_count = len(df_to_display)
            
            # --- è¡¨ç¤ºå½¢å¼ã‚’åˆ†å² ---
            if total_count == filtered_count:
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒã‹ã‹ã£ã¦ã„ãªã„å ´åˆ (ã¾ãŸã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒç·ä»¶æ•°ã¨ä¸€è‡´ã™ã‚‹å ´åˆ)
                display_text = f"{total_count}ä»¶ è¡¨ç¤º"
            else:
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒã‹ã‹ã£ã¦ã„ã‚‹å ´åˆ
                display_text = f"{filtered_count} / {total_count}ä»¶ è¡¨ç¤º"

            # â˜… HTML/CSSã§ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºã‚’å¤§ãã (1.1rem) ã—ã¦è¡¨ç¤º
            st.markdown(
                f"""
                <span style='font-size: 1.1rem; font-weight: bold; white-space: nowrap;'>
                {display_text}
                </span>
                """, 
                unsafe_allow_html=True
            )

        EXCEL_COLOR_MAP = {
            # 'ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹': (bg_color, font_color)
            'å…¬é–‹ä¸­': ('#22a579', '#FFFFFF'),
            'æœªç™»éŒ²': ('#111111', '#FFFFFF'),
            'å—ä»˜çµ‚äº†': ('#6c757d', '#FFFFFF'),
            'éè¡¨ç¤º': ('#6c757d', '#FFFFFF'),
            'åœ¨åº«0': ('#6c757d', '#FFFFFF'),
            'å€‰åº«': ('#6c757d', '#FFFFFF'),
            'æœªå—ä»˜': ('#ffc107', '#000000'), # æœªå—ä»˜ã¯é»’æ–‡å­—
            'è¦ç¢ºèª': ('#fa6c78', '#000000')  # è¦ç¢ºèªã¯é»’æ–‡å­—
        }

        # --- to_excel é–¢æ•° ---
        def to_excel(df):
            output = BytesIO()
            # XlsxWriter ã‚’ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦æŒ‡å®š
            with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                sheet_name = 'Sheet1'
                workbook = writer.book
                
                # --- 1. æ›¸å¼(ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)ã®å®šç¾© ---
                
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ›¸å¼ (ãƒ•ã‚©ãƒ³ãƒˆ: æ¸¸ã‚´ã‚·ãƒƒã‚¯)
                default_format = workbook.add_format({
                    'font_name': 'æ¸¸ã‚´ã‚·ãƒƒã‚¯'
                })

                # ãƒ˜ãƒƒãƒ€ãƒ¼æ›¸å¼ (ãƒ•ã‚©ãƒ³ãƒˆ: æ¸¸ã‚´ã‚·ãƒƒã‚¯, ç½«ç·šãªã—, å¤ªå­—)
                header_format = workbook.add_format({
                    'font_name': 'æ¸¸ã‚´ã‚·ãƒƒã‚¯',
                    'bold': True,
                    'border': 0  # ç½«ç·šãªã—
                })

                # è‰²ä»˜ãã‚»ãƒ«ã®æ›¸å¼ã‚’å‹•çš„ã«ä½œæˆ
                color_formats = {}
                for status, (bg_color, font_color) in EXCEL_COLOR_MAP.items():
                    color_formats[status] = workbook.add_format({
                        'font_name': 'æ¸¸ã‚´ã‚·ãƒƒã‚¯',
                        'bg_color': bg_color,
                        'font_color': font_color
                    })
                
                # --- 2. DataFrameã‚’Excelã«æ›¸ãè¾¼ã‚€ (ãƒ‡ãƒ¼ã‚¿ã®ã¿) ---
                # to_excelã§ãƒ‡ãƒ¼ã‚¿ã®ã¿æ›¸ãè¾¼ã‚€ (ãƒ˜ãƒƒãƒ€ãƒ¼ã¯å¾Œã§æ‰‹å‹•æç”»)
                df.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=1)
                
                # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
                worksheet = writer.sheets[sheet_name]

                # --- 3. ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ‰‹å‹•ã§æ›¸ãè¾¼ã‚€ (æ›¸å¼é©ç”¨ã®ãŸã‚) ---
                for col_num, value in enumerate(df.columns.values):
                    worksheet.write(0, col_num, value, header_format)

                # --- 4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ«ã«æ›¸å¼ã‚’é©ç”¨ ---
                # (to_excelã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ›¸å¼ã—ã‹é©ç”¨ã§ããªã„ãŸã‚ã€è‰²ä»˜ã‘ã®ãŸã‚ã«ä¸Šæ›¸ã)
                
                # PORTAL_ORDER ã¯ L.263 ä»˜è¿‘ã§å®šç¾©æ¸ˆã¿
                portal_cols = [p for p in PORTAL_ORDER if p in df.columns]
                check_col_name = 'ãƒã‚§ãƒƒã‚¯'
                
                # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆ
                for row_num in range(len(df)):
                    # ãƒ‡ãƒ¼ã‚¿è¡Œã®é–‹å§‹ã¯1è¡Œç›® (0è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼)
                    excel_row_idx = row_num + 1 
                    
                    # ã‚«ãƒ©ãƒ ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆ
                    for col_num, col_name in enumerate(df.columns):
                        value = df.iloc[row_num, col_num]
                        
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ›¸å¼ã‚’ã¾ãšé©ç”¨
                        cell_format = default_format
                        
                        # è‰²ä»˜ã‘å¯¾è±¡åˆ—ã‹åˆ¤å®š
                        if col_name in portal_cols and value in color_formats:
                            cell_format = color_formats.get(value, default_format)
                        elif col_name == check_col_name and value == 'è¦ç¢ºèª':
                            cell_format = color_formats.get('è¦ç¢ºèª', default_format)
                        
                        # ã‚»ãƒ«ã«å€¤ã¨æ›¸å¼ã‚’æ›¸ãè¾¼ã‚€
                        # (to_excelã§æ—¢ã«æ›¸ã‹ã‚ŒãŸå€¤ã‚’ä¸Šæ›¸ã)
                        worksheet.write(excel_row_idx, col_num, value, cell_format)

                # --- 5. åˆ—å¹…ã‚’è‡ªå‹•èª¿æ•´ ---
                # DataFrameã®åˆ—åã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã®è¾æ›¸ã‚’ä½œæˆ
                col_indices = {col_name: i for i, col_name in enumerate(df.columns)}

                # PORTAL_ORDER ã¯ L.263 ä»˜è¿‘ã§å®šç¾©æ¸ˆã¿
                portal_cols = [p for p in PORTAL_ORDER if p in col_indices]
                utility_cols = ['ãƒã‚§ãƒƒã‚¯', 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°', 'å…¬é–‹ä¸­ã®æ•°']

                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¹…
                default_width = 13 # (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ã‚„ã‚³ãƒ¼ãƒ‰ãªã©)

                # åˆ—ã”ã¨ã«å¹…ã‚’è¨­å®š
                for col_name, col_idx in col_indices.items():
                    width = default_width # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¹…ã‚’ã‚»ãƒƒãƒˆ
                    
                    if col_name == 'è¿”ç¤¼å“å':
                        # â˜… è¿”ç¤¼å“åã‚’ 60 ã«è¨­å®š (ç¾åœ¨ã®ç´„2/3ã‚’æƒ³å®š)
                        width = 60 
                    elif col_name == 'äº‹æ¥­è€…å':
                        # â˜… äº‹æ¥­è€…åã‚’ 25 ã«è¨­å®š (å°‘ã—åºƒã’ã‚‹)
                        width = 25 
                    elif col_name == 'äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰':
                        width = 15 # äº‹æ¥­è€…ã‚³ãƒ¼ãƒ‰ã¯å°‘ã—åºƒã‚
                    elif col_name not in portal_cols and col_name not in utility_cols:
                        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ—ä»¥å¤– (è¿”ç¤¼å“ã‚³ãƒ¼ãƒ‰ãªã©)
                        width = 15 
                    
                    # set_column(first_col, last_col, width)
                    worksheet.set_column(col_idx, col_idx, width)
                
            # writer.close() ã¯ with ãƒ–ãƒ­ãƒƒã‚¯ãŒè‡ªå‹•ã§å‡¦ç†
            return output.getvalue()

        # --- CSVå¤‰æ›é–¢æ•° ---
        @st.cache_data
        def to_csv(df):
            # DataFrameã‚’ã¾ãšCSVæ–‡å­—åˆ—ã«å¤‰æ›
            # (to_csvã«encodingã‚’æŒ‡å®šã—ã¦ã‚‚æ–‡å­—åˆ—å‡ºåŠ›ã§ã¯ç„¡è¦–ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯æŒ‡å®šã—ãªã„)
            csv_string = df.to_csv(index=False) 
            
            # æ–‡å­—åˆ—ã‚’ cp932 ãƒã‚¤ãƒˆåˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            # â˜… ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ããªã„æ–‡å­—ã¯ '?' ã«ç½®æ› (errors='replace')
            return csv_string.encode('cp932', errors='replace')

        with button_col:
            if not df_to_display.empty:
                
                # â˜… ã‚«ãƒ©ãƒ é–“ã®éš™é–“ã‚’ "small" (ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã¨åŒã˜) ã«è¨­å®š
                excel_col, csv_col = st.columns([1, 1], gap="small")

                # --- Excelä¿å­˜ãƒœã‚¿ãƒ³ã‚’1åˆ—ç›®ã«é…ç½® ---
                with excel_col:
                    excel_data = to_excel(df_to_display)
                    
                    # â˜… å¤‰æ›´: session_stateã‹ã‚‰å€¤ã‚’å–å¾—
                    # L708ã§ä¿å­˜ã—ãŸå€¤ã‚’ä½¿ç”¨ã€‚å­˜åœ¨ã—ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚‚è¨­å®š
                    base_portal_for_name = st.session_state.get('current_base_portal', 'N/A')
                    date_str_for_name = st.session_state.get('current_select_date_str', 'YYYYMMDD')
                    
                    # â˜… å¤‰æ›´: ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ–°ã—ã„å½¢å¼ã«
                    file_name_excel = f"æ²è¼‰çŠ¶æ³ãƒ‡ãƒ¼ã‚¿_{TODAY_STR}ï¼ˆtarget_{base_portal_for_name}_{date_str_for_name}ï¼‰.xlsx"
                    
                    st.download_button(
                        label="Excelä¿å­˜",
                        data=excel_data,
                        file_name=file_name_excel, # â˜… å¤‰æ›´
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key="excel_download",
                        width='stretch' 
                    )

                # --- CSVä¿å­˜ãƒœã‚¿ãƒ³ã‚’2åˆ—ç›®ã«é…ç½® ---
                with csv_col:
                    csv_data = to_csv(df_to_display)
                    
                    # â˜… å¤‰æ›´: session_stateã‹ã‚‰å€¤ã‚’å–å¾— (ä¸Šè¨˜ã¨åŒã˜å¤‰æ•°ã‚’ä½¿ç”¨)
                    base_portal_for_name = st.session_state.get('current_base_portal', 'N/A')
                    date_str_for_name = st.session_state.get('current_select_date_str', 'YYYYMMDD')
                    
                    # â˜… å¤‰æ›´: ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ–°ã—ã„å½¢å¼ã«
                    file_name_csv = f"æ²è¼‰çŠ¶æ³ãƒ‡ãƒ¼ã‚¿_{TODAY_STR}ï¼ˆtarget_{base_portal_for_name}_{date_str_for_name}ï¼‰.csv"
                    
                    st.download_button(
                        label="CSVä¿å­˜",
                        data=csv_data,
                        file_name=file_name_csv, # â˜… å¤‰æ›´
                        mime="text/csv",
                        key="csv_download",
                        width='stretch'
                    )

        # --- ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°ã¨è¡¨ç¤º ---
        color_map = {'å…¬é–‹ä¸­': 'background-color: #22a579; color: white;', 'æœªç™»éŒ²': 'background-color: #111111; color: white;', 'å—ä»˜çµ‚äº†': 'background-color: #6c757d; color: white;', 
                     'éè¡¨ç¤º': 'background-color: #6c757d; color: white;', 'åœ¨åº«0': 'background-color: #6c757d; color: white;', 'å€‰åº«': 'background-color: #6c757d; color: white;',
                     'æœªå—ä»˜': 'background-color: #ffc107; color: black;'}
        
        def style_dataframe(df):
            style = pd.DataFrame('', index=df.index, columns=df.columns)
            portal_cols = [p for p in PORTAL_ORDER if p in df.columns]
            for col in portal_cols: style[col] = df[col].map(color_map).fillna('')
            if 'ãƒã‚§ãƒƒã‚¯' in df.columns: style['ãƒã‚§ãƒƒã‚¯'] = df['ãƒã‚§ãƒƒã‚¯'].apply(lambda x: 'background-color: #fa6c78; color: black;' if x == 'è¦ç¢ºèª' else '')
            return style

        # --- ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ãƒ©ã‚¤ã‚¹ã¨æç”» ---
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒ0ä»¶ã§ãªã„å ´åˆã®ã¿ã‚¹ãƒ©ã‚¤ã‚¹ã¨æç”»ã‚’å®Ÿè¡Œ
        # (0ä»¶ã®å ´åˆã¯ L.1138 ã® st.warning ãŒè¡¨ç¤ºã•ã‚Œã‚‹æƒ³å®š)
        if not df_to_display.empty:
        
            # ãƒšãƒ¼ã‚¸ç•ªå·ã‹ã‚‰ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—
            # (current_page ã¯ L.1193 ã§è£œæ­£æ¸ˆã¿)
            start_idx = (current_page - 1) * ITEMS_PER_PAGE
            end_idx = start_idx + ITEMS_PER_PAGE
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹
            df_sliced = df_to_display.iloc[start_idx:end_idx]
            
            # â˜… ã‚¹ãƒ©ã‚¤ã‚¹ã—ãŸDFã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å…¨ä½“ã®é€£ç•ªã«å¤‰æ›´
            df_sliced.index = range(start_idx + 1, start_idx + 1 + len(df_sliced))
            
            # ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°å¯¾è±¡ã®ã‚«ãƒ©ãƒ ãƒªã‚¹ãƒˆ
            center_aligned_cols = [p for p in PORTAL_ORDER if p in df_sliced.columns] + ['ãƒã‚§ãƒƒã‚¯', 'å®šæœŸä¾¿ãƒ•ãƒ©ã‚°', 'å…¬é–‹ä¸­ã®æ•°']

            # â˜… ã‚¹ãƒ©ã‚¤ã‚¹ã—ãŸ df_sliced ã«å¯¾ã—ã¦ã‚¹ã‚¿ã‚¤ãƒªãƒ³ã‚°
            styler = df_sliced.style.apply(style_dataframe, axis=None).set_properties(subset=center_aligned_cols, **{'text-align': 'center'})

            # â˜… ã‚¹ãƒ©ã‚¤ã‚¹ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æç”»
            st.dataframe(styler, width='stretch', height=800)

        # --- ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³UI (è¡¨ã®ä¸‹ã«é…ç½®) ---
        # total_items, total_pages, current_page ã¯ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç›´å¾Œ(L.1184ä»˜è¿‘)ã§è¨ˆç®—æ¸ˆã¿
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœãŒ0ä»¶ã§ãªã„å ´åˆã®ã¿ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
        if total_items > 0:
            # (ITEMS_PER_PAGE, total_pages, current_page ã¯ L.1184 ä»˜è¿‘ã§å®šç¾©ãƒ»è¨ˆç®—æ¸ˆã¿)
            
            # [å…¨ä»¶æ•°è¡¨ç¤º] [ [n] / n ãƒšãƒ¼ã‚¸] 
            col_spacer, col_page_input, col_page_total, col_spacer_end, col_max_num = st.columns([
                3.5,  # ç©ºç™½ (èª¿æ•´)
                1.0, # [n] (å…¥åŠ›æ¬„)
                0.9, # / n ãƒšãƒ¼ã‚¸ (ãƒ†ã‚­ã‚¹ãƒˆ)
                1.0,  # ç©ºç™½ (èª¿æ•´)
                1.5  # æœ€å¤§ä»¶æ•°èª¬æ˜æ–‡
            ])

            # ãƒšãƒ¼ã‚¸ç•ªå·å…¥åŠ›ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
            # (st.number_input ã® on_change ã§å‘¼ã³å‡ºã•ã‚Œã‚‹)
            def update_page_number():
                # number_inputã®å€¤(page_input_box)ã‚’current_pageã«åæ˜ ã™ã‚‹
                if st.session_state.page_input_box != st.session_state.current_page:
                    st.session_state.current_page = st.session_state.page_input_box
                    # on_change ãŒç™ºç«ã™ã‚‹ã¨ Streamlit ãŒè‡ªå‹•ã§ rerun ã™ã‚‹ãŸã‚ã€st.rerun() ã¯ä¸è¦

            with col_page_input:
                # ãƒšãƒ¼ã‚¸ç•ªå·å…¥åŠ›æ¬„
                st.number_input(
                    label="ãƒšãƒ¼ã‚¸ç•ªå·",
                    min_value=1,
                    max_value=total_pages,
                    value=current_page, # â˜… è¡¨ç¤ºã™ã‚‹å€¤ã¯å¸¸ã« current_page
                    step=1,
                    key="page_input_box", # â˜… key ã‚’ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å‚ç…§ç”¨ã®åˆ¥åã«å¤‰æ›´
                    on_change=update_page_number, # å¤‰æ›´æ™‚ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
                    label_visibility="collapsed",
                    help=f"1ï½{total_pages} ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’å…¥åŠ›"
                )

            with col_page_total:
                # ç·ãƒšãƒ¼ã‚¸æ•°è¡¨ç¤º (å·¦æƒãˆã«ã—ã¦å…¥åŠ›æ¬„ã®ã™ãå³ã«é…ç½®)
                st.markdown(
                    f"<div style='margin-top: 8px; text-align: left; font-weight: bold;'> / {total_pages} ãƒšãƒ¼ã‚¸</div>",
                    unsafe_allow_html=True
                )
            
            with col_max_num:
                # ãƒšãƒ¼ã‚¸ç•ªå·è¡¨ç¤º (ä¸­å¤®æƒãˆ)
                st.markdown(
                    f"<div style='margin-top: 8px; text-align: center; font-weight: 500;'>â€»1ãƒšãƒ¼ã‚¸æœ€å¤§500ä»¶è¡¨ç¤º</div>",
                    unsafe_allow_html=True
                )

        st.markdown("---")
        
        # ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ã®ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°
        if 'confirming_reset' not in st.session_state:
            st.session_state.confirming_reset = False

        if st.session_state.confirming_reset:
            st.warning("æœ¬å½“ã«æ²è¼‰çŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ")
            
            col1, col2, _ = st.columns([1, 1, 8], gap="small") 
            
            with col1:
                if st.button("OK", key="reset_confirm_ok", width='stretch'):
                    # å®Ÿè¡Œå‡¦ç†
                    if 'dataframes' in st.session_state:
                        meta_keys = [k for k in st.session_state.dataframes if k.endswith('_metadata')]
                        for k in meta_keys:
                            del st.session_state.dataframes[k] # è¾æ›¸ã®ä¸­èº«ã‚’å‰Šé™¤

                    keys_to_clear = [
                        'results_df', 'dataframes', 'choice_stock_processed', 'rakuten_merged',
                        'current_select_date_str', 'current_base_portal',
                        'f_search', 'f_vendor', 'f_check', 'f_teiki' # â˜… ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã‚‚ã‚¯ãƒªã‚¢
                    ]
                    for key in keys_to_clear:
                        if key in st.session_state:
                            del st.session_state[key] # å±æ€§è‡ªä½“ã‚’å‰Šé™¤

                    st.session_state.uploader_key += 1
                    
                    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã®ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                    st.session_state.show_reset_success = True
                    
                    # çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ã—ã¦å†æç”»
                    st.session_state.confirming_reset = False
                    st.rerun()
            with col2:
                if st.button("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", key="reset_confirm_cancel", width='stretch'):
                    st.session_state.confirming_reset = False
                    st.rerun()
        else:
            if st.button("æ²è¼‰çŠ¶æ³ã‚’ãƒªã‚»ãƒƒãƒˆ"):
                st.session_state.confirming_reset = True
                st.rerun()